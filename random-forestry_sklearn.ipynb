{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bcc3fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment Overview\n",
    "\n",
    "In this exercise, we've some goals we're seeking to accomplish.\n",
    "\n",
    "1. Pull in the experimental version of RForestry built for Python\n",
    "2. Setup a sklearn pipeline which requires the use of a Ray cluster\n",
    "3. Run distributed work across a Ray cluster which leverages RForestry's fit() method\n",
    "4. Link to the outputs written to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2967574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11150505389719978\n",
      "--- 0.010935783386230469 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from random_forestry import RandomForest\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "df['target'] = data['target']\n",
    "X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "y = df['sepal length (cm)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "fr = RandomForest(ntree = 100)\n",
    "\n",
    "start_time = time.time()\n",
    "fr.fit(X_train, y_train)\n",
    "forest_preds = fr.predict(X_test)\n",
    "print(fr.score(X_test, y_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cd98af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ain't gonna work as-is. Food for thought to upstream contributors. Let's fix it.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "try:\n",
    "    check_estimator(RandomForest())\n",
    "except TypeError:\n",
    "    print(\"Ain't gonna work as-is. Food for thought to upstream contributors. Let's fix it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a3d8a",
   "metadata": {},
   "source": [
    "## Modified Version of RForestry for sklearn\n",
    "\n",
    "Below is the scrappy modification of the RForestry package to \"work\" with sklearn. There are aspects here that are most certainly not correct, but for demonstration purpsoses deemed good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43635668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/developers/develop.html\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "import dataclasses\n",
    "import math\n",
    "import os\n",
    "import pickle  # nosec B403 - 'Consider possible security implications associated with pickle'\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from random import randrange\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, Final\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from pydantic import (  # pylint: disable=no-name-in-module\n",
    "    ConfigDict,\n",
    "    StrictBool,\n",
    "    StrictFloat,\n",
    "    StrictInt,\n",
    "    confloat,\n",
    "    conint,\n",
    "    validator,\n",
    ")\n",
    "from pydantic.dataclasses import dataclass\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from random_forestry import RandomForest\n",
    "from random_forestry import extension\n",
    "\n",
    "@dataclass\n",
    "class ProcessedDta:  # pylint: disable=too-many-instance-attributes\n",
    "    processed_x: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    y: np.ndarray = field(default_factory=lambda: np.array(0))\n",
    "    categorical_feature_cols: np.ndarray = field(default_factory=lambda: np.array(0))\n",
    "    categorical_feature_mapping: List[Dict[Any, Any]] = field(default_factory=list)\n",
    "    feature_weights: Optional[np.ndarray] = None\n",
    "    feature_weights_variables: Optional[np.ndarray] = None\n",
    "    deep_feature_weights: Optional[np.ndarray] = None\n",
    "    deep_feature_weights_variables: Optional[str] = None\n",
    "    observation_weights: Optional[str] = None\n",
    "    monotonic_constraints: Optional[str] = None\n",
    "    linear_feature_cols: np.ndarray = field(default_factory=lambda: np.array(0))\n",
    "    groups_mapping: Optional[Dict[str, Any]] = None\n",
    "    groups: Optional[str] = None\n",
    "    col_means: np.ndarray = field(default_factory=lambda: np.array(0))\n",
    "    col_sd: np.ndarray = field(default_factory=lambda: np.array(0))\n",
    "    has_nas: bool = False\n",
    "    na_direction: bool = False\n",
    "    n_observations: int = 0\n",
    "    num_columns: int = 0\n",
    "    feat_names: Optional[np.ndarray] = None\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "def has_nas(x: pd.DataFrame) -> bool:\n",
    "    return x.isnull().values.any()\n",
    "\n",
    "\n",
    "def get_sampsize(forest, x: pd.DataFrame) -> int:\n",
    "    nrows, _ = x.shape\n",
    "    if forest.sampsize is None:\n",
    "        sampsize = nrows if forest.replace else math.ceil(0.632 * nrows)\n",
    "\n",
    "    # only if sample.fraction is given, update sampsize\n",
    "    if forest.sample_fraction is not None:\n",
    "        sampsize = math.ceil(forest.sample_fraction * nrows)\n",
    "\n",
    "    return sampsize\n",
    "\n",
    "\n",
    "def get_mtry(forest, x: pd.DataFrame) -> int:\n",
    "    _, ncols = x.shape\n",
    "    if forest.mtry is None:\n",
    "        return max((ncols // 3), 1)\n",
    "    return forest.mtry\n",
    "\n",
    "\n",
    "def get_feat_names(x: Union[pd.DataFrame, pd.Series, List]) -> Optional[np.ndarray]:\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        return x.columns.values\n",
    "    if type(x).__module__ == np.__name__ or isinstance(x, (list, pd.Series)):\n",
    "        print(\n",
    "            \"x does not have column names. \",\n",
    "            \"The check that columns are provided in the same order when training and predicting will be skipped\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    raise AttributeError(\"x must be a Pandas DataFrame, a numpy array, a Pandas Series, or a regular list\")\n",
    "\n",
    "\n",
    "def find_match(arr_a: Union[np.ndarray, List], arr_b: Union[np.ndarray, List]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    --------------------------------------\n",
    "    Helper Function\n",
    "    @return a nunpy array indicating the indices of first occurunces of\n",
    "      the elements of arr_a in arr_b\n",
    "    \"\"\"\n",
    "\n",
    "    temp_dict = {}\n",
    "\n",
    "    for index, element in enumerate(arr_b):\n",
    "        if str(element) not in temp_dict:\n",
    "            temp_dict[str(arr_b[index])] = index\n",
    "\n",
    "    return np.array([temp_dict[str(val)] for val in arr_a])\n",
    "\n",
    "\n",
    "def forest_checker(forest) -> None:\n",
    "    \"\"\"\n",
    "    Checks if RandomForest object has valid pointer for C++ object.\n",
    "    @param object a RandomForest object\n",
    "    @return A message if the forest does not have a valid C++ pointer.\n",
    "    \"\"\"\n",
    "\n",
    "    if (not forest.dataframe) or (not forest.forest):\n",
    "        raise ValueError(\"The RandomForest object has invalid ctypes pointers.\")\n",
    "\n",
    "\n",
    "# Given a dataframe with Y and Y.hat at least, fits an OLS and gives the LOO\n",
    "# predictions on the sample\n",
    "def loo_pred_helper(data_frame: pd.DataFrame) -> dict:\n",
    "\n",
    "    Y = data_frame[\"Y\"]\n",
    "    X = data_frame.loc[:, data_frame.columns != \"Y\"]\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    adjust_lm = sm.OLS(Y, X).fit()\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_pred = np.empty(Y.size)\n",
    "\n",
    "    for i, (train, test) in enumerate(cv.split(X)):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train, :], X.iloc[test, :]\n",
    "        y_train, _ = Y[train], Y[test]\n",
    "\n",
    "        # fit model\n",
    "        model = sm.OLS(y_train, X_train).fit()\n",
    "        cv_pred[i] = model.predict(X_test)\n",
    "\n",
    "    return {\"insample_preds\": cv_pred, \"adjustment_model\": adjust_lm}\n",
    "\n",
    "\n",
    "def preprocess_training(x: pd.DataFrame, y) -> Tuple[pd.DataFrame, np.ndarray, List[Dict]]:\n",
    "    \"\"\"\n",
    "    -- Methods for Preprocessing Data --------------------------------------------\n",
    "    @title preprocess_training\n",
    "    @description Perform preprocessing for the training data, including\n",
    "      converting data to dataframe, and encoding categorical data into numerical\n",
    "      representation.\n",
    "    @inheritParams RandomForest\n",
    "    @return A list of two datasets along with necessary information that encodes\n",
    "      the preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the input dimension of x matches y\n",
    "    if len(x.index) != y.size:\n",
    "        raise ValueError(\"The dimension of input dataset x doesn't match the output vector y.\")\n",
    "\n",
    "    # Track the order of all features\n",
    "    feature_names = x.columns.values\n",
    "    if feature_names.size == 0:\n",
    "        warnings.warn(\"No names are given for each column.\")\n",
    "\n",
    "    # Track all categorical features (both factors and characters)\n",
    "    categorical_feature_cols = np.array((x.select_dtypes(\"category\")).columns)\n",
    "    feature_character_cols = np.array((x.select_dtypes(\"object\")).columns)\n",
    "\n",
    "    if feature_character_cols.size != 0:  # convert to a factor\n",
    "        warnings.warn(\"Character value features will be cast to categorical data.\")\n",
    "        categorical_feature_cols = np.concatenate((categorical_feature_cols, feature_character_cols), axis=0)\n",
    "\n",
    "    categorical_feature_cols = x.columns.get_indexer(categorical_feature_cols)\n",
    "\n",
    "    # For each categorical feature, encode x into numeric representation and\n",
    "    # save the encoding mapping\n",
    "    categorical_feature_mapping: List[Dict] = []\n",
    "    for categorical_feature_col in categorical_feature_cols:\n",
    "        x.iloc[:, categorical_feature_col] = pd.Series(\n",
    "            x.iloc[:, categorical_feature_col], dtype=\"category\"\n",
    "        ).cat.remove_unused_categories()\n",
    "\n",
    "        categorical_feature_mapping.append(\n",
    "            {\n",
    "                \"categoricalFeatureCol\": categorical_feature_col,\n",
    "                \"uniqueFeatureValues\": list(x.iloc[:, categorical_feature_col].cat.categories),\n",
    "                \"numericFeatureValues\": np.arange(len(x.iloc[:, categorical_feature_col].cat.categories)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        x.iloc[:, categorical_feature_col] = pd.Series(x.iloc[:, categorical_feature_col].cat.codes, dtype=\"category\")\n",
    "\n",
    "    return x, categorical_feature_cols, categorical_feature_mapping\n",
    "\n",
    "\n",
    "def preprocess_testing(x, categorical_feature_cols: np.ndarray, categorical_feature_mapping: List[Dict]) -> Any:\n",
    "    \"\"\"\n",
    "    @title preprocess_testing\n",
    "    @description Perform preprocessing for the testing data, including converting\n",
    "      data to dataframe, and testing if the columns are consistent with the\n",
    "      training data and encoding categorical data into numerical representation\n",
    "      in the same way as training data.\n",
    "    @inheritParams RandomForest\n",
    "    @param categorical_feature_cols A list of index for all categorical data. Used\n",
    "      for trees to detect categorical columns.\n",
    "    @param categorical_feature_mapping A list of encoding details for each\n",
    "      categorical column, including all unique factor values and their\n",
    "      corresponding numeric representation.\n",
    "    @return A preprocessed training dataaset x\n",
    "    \"\"\"\n",
    "\n",
    "    # Track the order of all features\n",
    "    testing_feature_names = x.columns.values\n",
    "    if testing_feature_names.size == 0:\n",
    "        warnings.warn(\"No names are given for each column.\")\n",
    "\n",
    "    # Track all categorical features (both factors and characters)\n",
    "    feature_factor_cols = np.array((x.select_dtypes(\"category\")).columns)\n",
    "    feature_character_cols = np.array((x.select_dtypes(\"object\")).columns)\n",
    "\n",
    "    testing_categorical_feature_cols = np.concatenate((feature_factor_cols, feature_character_cols), axis=0)\n",
    "    testing_categorical_feature_cols = x.columns.get_indexer(testing_categorical_feature_cols)\n",
    "\n",
    "    if (set(categorical_feature_cols) - set(testing_categorical_feature_cols)) or (\n",
    "        set(testing_categorical_feature_cols) - set(categorical_feature_cols)\n",
    "    ):\n",
    "        raise ValueError(\"Categorical columns are different between testing and training data.\")\n",
    "\n",
    "    # For each categorical feature, encode x into numeric representation\n",
    "    for categorical_feature_mapping_ in categorical_feature_mapping:\n",
    "        categorical_feature_col = categorical_feature_mapping_[\"categoricalFeatureCol\"]\n",
    "        # Get all unique feature values\n",
    "        testing_unique_feature_values = x.iloc[:, categorical_feature_col].unique()\n",
    "        unique_feature_values = categorical_feature_mapping_[\"uniqueFeatureValues\"]\n",
    "        numeric_feature_values = categorical_feature_mapping_[\"numericFeatureValues\"]\n",
    "\n",
    "        # If testing dataset contains more, adding new factors to the mapping list\n",
    "        diff_unique_feature_values = set(testing_unique_feature_values) - set(unique_feature_values)\n",
    "        if diff_unique_feature_values:\n",
    "            unique_feature_values = np.concatenate(\n",
    "                (list(unique_feature_values), list(diff_unique_feature_values)), axis=0\n",
    "            )\n",
    "            numeric_feature_values = np.arange(unique_feature_values.size)\n",
    "\n",
    "            # update\n",
    "            categorical_feature_mapping_[\"uniqueFeatureValues\"] = unique_feature_values\n",
    "            categorical_feature_mapping_[\"numericFeatureValues\"] = numeric_feature_values\n",
    "\n",
    "        x.iloc[:, categorical_feature_col] = pd.Series(\n",
    "            find_match(x.iloc[:, categorical_feature_col], unique_feature_values),\n",
    "            dtype=\"category\",\n",
    "        )\n",
    "\n",
    "    # Return transformed data and encoding information\n",
    "    return x\n",
    "\n",
    "\n",
    "def scale_center(\n",
    "    x: pd.DataFrame, categorical_feature_cols: np.ndarray, col_means: np.ndarray, col_sd: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    @title scale_center\n",
    "    @description Given a dataframe, scale and center the continous features\n",
    "    @param x A dataframe in order to be processed.\n",
    "    @param categoricalFeatureCols A vector of the categorical features, we\n",
    "      don't want to scale/center these.\n",
    "    @param colMeans A vector of the means to center each column.\n",
    "    @param colSd A vector of the standard deviations to scale each column with.\n",
    "    @return A scaled and centered  dataset x\n",
    "    \"\"\"\n",
    "\n",
    "    for col_idx in range(len(x.columns)):\n",
    "        if col_idx not in categorical_feature_cols:\n",
    "            if col_sd[col_idx] != 0:\n",
    "                x.iloc[:, col_idx] = (x.iloc[:, col_idx] - col_means[col_idx]) / col_sd[col_idx]\n",
    "            else:\n",
    "                x.iloc[:, col_idx] = x.iloc[:, col_idx] - col_means[col_idx]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unscale_uncenter(x: pd.DataFrame, categorical_feature_cols: list, col_means: list, col_sd: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    @title unscale_uncenter\n",
    "    @description Given a dataframe, un scale and un center the continous features\n",
    "    @param x A dataframe in order to be processed.\n",
    "    @param categoricalFeatureCols A vector of the categorical features, we\n",
    "      don't want to scale/center these. Should be 1-indexed.\n",
    "    @param colMeans A vector of the means to add to each column.\n",
    "    @param colSd A vector of the standard deviations to rescale each column with.\n",
    "    @return A dataset x in it's original scaling\n",
    "    \"\"\"\n",
    "\n",
    "    for index, column in x.columns:\n",
    "        if column not in categorical_feature_cols:\n",
    "            if col_sd[index] != 0:\n",
    "                x.iloc[:, index] = x.iloc[:, index] * col_sd[index] + col_means[index]\n",
    "            else:\n",
    "                x.iloc[:, index] = x.iloc[:, index] + col_means[index]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def scale(x, y, processed_x, categorical_feature_cols):\n",
    "    _, ncol = x.shape\n",
    "    col_means = np.repeat(0.0, ncol + 1)\n",
    "    col_sd = np.repeat(0.0, ncol + 1)\n",
    "\n",
    "    for col_idx in range(ncol):\n",
    "        if col_idx not in categorical_feature_cols:\n",
    "            col_means[col_idx] = np.nanmean(processed_x.iloc[:, col_idx])\n",
    "            col_sd[col_idx] = np.nanstd(processed_x.iloc[:, col_idx])\n",
    "\n",
    "    # Scale columns of X\n",
    "    processed_x = scale_center(processed_x, categorical_feature_cols, col_means, col_sd)\n",
    "\n",
    "    # Center and scale Y\n",
    "    col_means[ncol] = np.nanmean(y)\n",
    "    col_sd[ncol] = np.nanstd(y)\n",
    "    if col_sd[ncol] != 0:\n",
    "        y = (y - col_means[ncol]) / col_sd[ncol]\n",
    "    else:\n",
    "        y = y - col_means[ncol]\n",
    "\n",
    "    return processed_x, y, col_means, col_sd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import functools\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class BaseValidator(ABC):\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "\n",
    "    def __get__(self, obj, _):\n",
    "        return functools.partial(self.__call__, obj)\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        ...\n",
    "\n",
    "\n",
    "class FitValidator(BaseValidator):\n",
    "\n",
    "    def validate_monotonic_constraints(self, *args, **kwargs):\n",
    "        _self = args[0]\n",
    "\n",
    "        x = pd.DataFrame(kwargs.get(\"x\", args[1])).copy()\n",
    "        _, ncols = x.shape\n",
    "\n",
    "        if \"monotonic_constraints\" not in kwargs:\n",
    "            monotonic_constraints = np.zeros(ncols, dtype=np.intc)\n",
    "        else:\n",
    "            monotonic_constraints = np.array(kwargs[\"monotonic_constraints\"], dtype=np.intc)\n",
    "\n",
    "        if monotonic_constraints.size != ncols:\n",
    "            raise ValueError(\"monotonic_constraints must have the size of x\")\n",
    "        if any(i not in (0, 1, -1) for i in monotonic_constraints):\n",
    "            raise ValueError(\"monotonic_constraints must be either 1, 0, or -1\")\n",
    "        if any(i != 0 for i in monotonic_constraints) and _self.linear:\n",
    "            raise ValueError(\"Cannot use linear splitting with monotonic_constraints\")\n",
    "\n",
    "        return monotonic_constraints\n",
    "\n",
    "    def validate_observation_weights(self, *args, **kwargs):\n",
    "        _self = args[0]\n",
    "\n",
    "        x = pd.DataFrame(kwargs.get(\"x\", args[1])).copy()\n",
    "        nrows, _ = x.shape\n",
    "\n",
    "        if not _self.replace:\n",
    "            observation_weights = np.zeros(nrows, dtype=np.double)\n",
    "        elif \"observation_weights\" not in kwargs:\n",
    "            observation_weights = np.repeat(1.0, nrows)\n",
    "        else:\n",
    "            observation_weights = np.array(kwargs[\"observation_weights\"], dtype=np.double)\n",
    "\n",
    "        if observation_weights.size != nrows:\n",
    "            raise ValueError(\"observation_weights must have length len(x)\")\n",
    "        if any(i < 0 for i in observation_weights):\n",
    "            raise ValueError(\"The entries in observation_weights must be non negative\")\n",
    "        if _self.replace and np.sum(observation_weights) == 0:\n",
    "            raise ValueError(\"There must be at least one non-zero weight in observation_weights\")\n",
    "\n",
    "        return observation_weights\n",
    "\n",
    "    def validate_lin_feats(self, *args, **kwargs):\n",
    "        x = pd.DataFrame(kwargs.get(\"x\", args[1])).copy()\n",
    "        _, ncols = x.shape\n",
    "\n",
    "        if \"lin_feats\" not in kwargs:\n",
    "            lin_feats = np.arange(ncols, dtype=np.ulonglong)\n",
    "        else:\n",
    "            lin_feats = pd.unique(np.array(kwargs[\"lin_feats\"], dtype=np.ulonglong))\n",
    "\n",
    "        if any(i < 0 or i >= ncols for i in lin_feats):\n",
    "            raise ValueError(\"lin_feats must contain positive integers less than len(x.columns).\")\n",
    "\n",
    "        return lin_feats\n",
    "\n",
    "    def validate_feature_weights(self, *args, **kwargs):\n",
    "        x = pd.DataFrame(kwargs.get(\"x\", args[1])).copy()\n",
    "        _, ncols = x.shape\n",
    "\n",
    "        if \"feature_weights\" not in kwargs:\n",
    "            feature_weights = np.repeat(1.0, ncols)\n",
    "            interaction_variables = [] if \"interaction_variables\" not in kwargs else kwargs[\"interaction_variables\"]\n",
    "            feature_weights[interaction_variables] = 0.0\n",
    "        else:\n",
    "            feature_weights = np.array(kwargs[\"feature_weights\"], dtype=np.double)\n",
    "\n",
    "        if feature_weights.size != ncols:\n",
    "            raise ValueError(\"feature_weights must have length len(x.columns)\")\n",
    "\n",
    "        if any(i < 0 for i in feature_weights):\n",
    "            raise ValueError(\"The entries in feature_weights must be non negative\")\n",
    "\n",
    "        if np.sum(feature_weights) == 0:\n",
    "            raise ValueError(\"There must be at least one non-zero weight in feature_weights\")\n",
    "\n",
    "        return feature_weights\n",
    "\n",
    "    def validate_deep_feature_weights(self, *args, **kwargs):\n",
    "        x = pd.DataFrame(kwargs.get(\"x\", args[1])).copy()\n",
    "        _, ncols = x.shape\n",
    "\n",
    "        if \"deep_feature_weights\" not in kwargs:\n",
    "            deep_feature_weights = np.repeat(1.0, ncols)\n",
    "        else:\n",
    "            deep_feature_weights = np.array(kwargs[\"deep_feature_weights\"], dtype=np.double)\n",
    "\n",
    "        if deep_feature_weights.size != ncols:\n",
    "            raise ValueError(\"deep_feature_weights must have length len(x.columns)\")\n",
    "\n",
    "        if any(i < 0 for i in deep_feature_weights):\n",
    "            raise ValueError(\"The entries in deep_feature_weights must be non negative\")\n",
    "\n",
    "        if np.sum(deep_feature_weights) == 0:\n",
    "            raise ValueError(\"There must be at least one non-zero weight in deep_feature_weights\")\n",
    "\n",
    "        return deep_feature_weights\n",
    "\n",
    "    def validate_groups(self, *_, **kwargs):\n",
    "        if \"groups\" in kwargs:\n",
    "            groups = kwargs[\"groups\"]\n",
    "            if not pd.api.types.is_categorical_dtype(groups):\n",
    "                raise ValueError(\n",
    "                    \"groups must have a data dtype of categorical. \",\n",
    "                    'Try using pd.Categorical(...) or pd.Series(..., dtype=\"category\").',\n",
    "                )\n",
    "            if len(groups.unique()) == 1:\n",
    "                raise ValueError(\"groups must have more than 1 level to be left out from sampling.\")\n",
    "\n",
    "            return pd.Series(groups, dtype=\"category\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        _self = args[0]\n",
    "\n",
    "        x = pd.DataFrame(kwargs.get(\"x\", args[1])).copy()\n",
    "        y = np.array(kwargs.get(\"y\", args[1] if \"x\" in kwargs else args[2]), dtype=np.double).copy()\n",
    "        nrows, ncols = x.shape\n",
    "\n",
    "        # Check if the input dimension of x matches y\n",
    "        if nrows != y.size:\n",
    "            raise ValueError(\"The dimension of input dataset x doesn't match the output y.\")\n",
    "\n",
    "        if np.isnan(y).any():\n",
    "            raise ValueError(\"y contains missing data.\")\n",
    "\n",
    "        if _self.linear and x.isnull().values.any():\n",
    "            raise ValueError(\"Cannot do imputation splitting with linear.\")\n",
    "\n",
    "        if not _self.replace and get_sampsize(_self, x) > nrows:\n",
    "            raise ValueError(\"You cannot sample without replacement with size more than total number of observations.\")\n",
    "        if get_mtry(_self, x) > ncols:\n",
    "            raise ValueError(\"mtry cannot exceed total amount of features in x.\")\n",
    "\n",
    "        kwargs[\"monotonic_constraints\"] = self.validate_monotonic_constraints(*args, **kwargs)\n",
    "\n",
    "        kwargs[\"lin_feats\"] = self.validate_lin_feats(*args, **kwargs)\n",
    "\n",
    "        kwargs[\"feature_weights\"] = self.validate_feature_weights(*args, **kwargs)\n",
    "\n",
    "        kwargs[\"deep_feature_weights\"] = self.validate_deep_feature_weights(*args, **kwargs)\n",
    "\n",
    "        kwargs[\"observation_weights\"] = self.validate_observation_weights(*args, **kwargs)\n",
    "\n",
    "        if \"groups\" in kwargs:\n",
    "            kwargs[\"groups\"] = self.validate_groups(*args, **kwargs)\n",
    "\n",
    "        return self.function(*args, **kwargs)\n",
    "\n",
    "class PredictValidator(BaseValidator):\n",
    "\n",
    "    DEFAULT_NEWDATA: Final = None\n",
    "    DEFAULT_AGGREGATION: Final[str] = \"average\"\n",
    "\n",
    "    def get_newdata(self, *args, **kwargs) -> Union[pd.DataFrame, pd.Series, List, None]:\n",
    "        if len(args) > 2:\n",
    "            raise TypeError(f\"predict() takes from 1 to 2 positional arguments but {len(args)} were given\")\n",
    "        if len(args) == 2:\n",
    "            if \"X\" in kwargs or \"newdata\" in kwargs:\n",
    "                raise AttributeError(\"newdata specified both in args and kwargs\")\n",
    "            return args[1]\n",
    "        return kwargs.get(\"newdata\", kwargs.get(\"X\", __class__.DEFAULT_NEWDATA))\n",
    "\n",
    "    def validate_newdata(self, *args, **kwargs) -> pd.DataFrame:\n",
    "        _self = args[0]\n",
    "        newdata = self.get_newdata(*args, **kwargs)\n",
    "\n",
    "        if newdata is not None:\n",
    "            if not (isinstance(newdata, (pd.DataFrame, pd.Series, list)) or type(newdata).__module__ == np.__name__):\n",
    "                raise AttributeError(\n",
    "                    \"newdata must be a Pandas DataFrame, a numpy array, a Pandas Series, or a regular list\"\n",
    "                )\n",
    "\n",
    "            newdata = (pd.DataFrame(newdata)).copy()\n",
    "            newdata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if len(newdata.columns) != _self.processed_dta.num_columns:\n",
    "                raise ValueError(\n",
    "                    f\"newdata has {len(newdata.columns)}, \"\n",
    "                    f\"but the forest was trained with {_self.processed_dta.num_columns} columns.\"\n",
    "                )\n",
    "\n",
    "            if _self.processed_dta.feat_names is not None:\n",
    "                if not set(newdata.columns) == set(_self.processed_dta.feat_names):\n",
    "                    # TODO FIX THIS\n",
    "                    newdata.columns = _self.processed_dta.feat_names\n",
    "\n",
    "                # If linear is true we can't predict observations with some features missing.\n",
    "                if _self.linear and newdata.isnull().values.any():\n",
    "                    raise ValueError(\"linear does not support missing data\")\n",
    "\n",
    "                if not all(newdata.columns == _self.processed_dta.feat_names):\n",
    "                    warnings.warn(\"newdata columns have been reordered so that they match the training feature matrix\")\n",
    "                    newdata = newdata[_self.processed_dta.feat_names]\n",
    "\n",
    "        return newdata\n",
    "\n",
    "    def validate_exact(self, **kwargs) -> bool:\n",
    "        if \"exact\" in kwargs:\n",
    "            return kwargs[\"exact\"]\n",
    "        if kwargs[\"newdata\"] is None:\n",
    "            return True\n",
    "        if len(kwargs[\"newdata\"].index) > 1e5:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_trees(self, *args, **kwargs) -> None:\n",
    "        _self = args[0]\n",
    "\n",
    "        if \"trees\" in kwargs:\n",
    "            if not kwargs[\"exact\"] or kwargs[\"aggregation\"] != \"average\":\n",
    "                raise ValueError(\"When using tree indices, we must have exact = True and aggregation = 'average' \")\n",
    "\n",
    "            if any(\n",
    "                (not isinstance(i, (int, np.integer))) or (i < -_self.ntree) or (i >= _self.ntree)\n",
    "                for i in kwargs[\"trees\"]\n",
    "            ):\n",
    "                raise ValueError(\"trees must contain indices which are integers between -ntree and ntree-1\")\n",
    "\n",
    "    def validate_aggregation(self, *args, **kwargs) -> str:\n",
    "        _self = args[0]\n",
    "\n",
    "        aggregation = kwargs.get(\"aggregation\", __class__.DEFAULT_AGGREGATION)\n",
    "\n",
    "        if aggregation == \"oob\":\n",
    "            pass\n",
    "        elif aggregation == \"doubleOOB\":\n",
    "            if not _self.double_bootstrap:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to do double OOB predictions \"\n",
    "                    \"with a forest that was not trained with doubleBootstrap = True\"\n",
    "                )\n",
    "        elif aggregation == \"coefs\":\n",
    "            if not _self.linear:\n",
    "                raise ValueError(\"Aggregation can only be linear with setting the parameter linear = True.\")\n",
    "            if kwargs[\"newdata\"] is None:\n",
    "                raise ValueError(\"When using an aggregation that is not oob or doubleOOB, one must supply newdata\")\n",
    "        else:\n",
    "            if kwargs[\"newdata\"] is None:\n",
    "                raise ValueError(\"When using an aggregation that is not oob or doubleOOB, one must supply newdata\")\n",
    "\n",
    "        return aggregation\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        _self = args[0]\n",
    "\n",
    "        forest_checker(_self)\n",
    "\n",
    "        kwargs[\"newdata\"] = self.validate_newdata(*args, **kwargs)\n",
    "\n",
    "        kwargs[\"exact\"] = self.validate_exact(**kwargs)\n",
    "\n",
    "        kwargs[\"aggregation\"] = self.validate_aggregation(*args, **kwargs)\n",
    "\n",
    "        kwargs[\"nthread\"] = kwargs.get(\"nthread\", _self.nthread)\n",
    "\n",
    "        self.validate_trees(*args, **kwargs)\n",
    "\n",
    "        return self.function(_self, **kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "class RandomForestEstimator(ClassifierMixin, BaseEstimator, RandomForest):\n",
    "    \n",
    "    def _more_tags(self):\n",
    "        # https://scikit-learn.org/stable/developers/develop.html#estimator-tags\n",
    "        return {\n",
    "            \"multioutput_only\": True,\n",
    "            \"requires_y\": True,\n",
    "            \"X_types\": [\"1darray\"]\n",
    "        }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntree: conint(gt=0, strict=True) = 500,\n",
    "        replace: StrictBool = True,\n",
    "        sampsize: Optional[conint(gt=0, strict=True)] = None,\n",
    "        sample_fraction: Optional[Union[conint(gt=0, strict=True), confloat(gt=0, strict=True)]] = None,\n",
    "        mtry: Optional[conint(gt=0, strict=True)] = None,\n",
    "        nodesize_spl: conint(gt=0, strict=True) = 5,\n",
    "        nodesize_avg: conint(gt=0, strict=True) = 5,\n",
    "        nodesize_strict_spl: conint(gt=0, strict=True) = 1,\n",
    "        nodesize_strict_avg: conint(gt=0, strict=True) = 1,\n",
    "        min_split_gain: confloat(ge=0) = 0,\n",
    "        max_depth: Optional[conint(gt=0, strict=True)] = None,\n",
    "        interaction_depth: Optional[conint(gt=0, strict=True)] = None ,\n",
    "        splitratio: confloat(ge=0, le=1) = 1.0,\n",
    "        oob_honest: StrictBool = False,\n",
    "        double_bootstrap: Optional[StrictBool] = None,\n",
    "        seed: conint(ge=0, strict=True) = randrange(1001),\n",
    "        verbose: StrictBool = False,\n",
    "        nthread: conint(ge=0, strict=True) = 0,\n",
    "        splitrule: str = \"variance\",\n",
    "        middle_split: StrictBool = False,\n",
    "        max_obs: Optional[conint(gt=0, strict=True)] = None,\n",
    "        linear: StrictBool = False,\n",
    "        min_trees_per_fold: conint(ge=0, strict=True) = 0,\n",
    "        fold_size: conint(gt=0, strict=True) = 1,\n",
    "        monotone_avg: StrictBool = False,\n",
    "        overfit_penalty: Union[StrictInt, StrictFloat] = 1,\n",
    "        scale: StrictBool = False,\n",
    "        double_tree: StrictBool = False,\n",
    "        na_direction: StrictBool = False,\n",
    "        forest: Optional[pd.DataFrame] = dataclasses.field(default=None, init=False),\n",
    "        dataframe: Optional[pd.DataFrame] = dataclasses.field(default=None, init=False),\n",
    "        processed_dta: Optional[ProcessedDta] = dataclasses.field(default=None, init=False),\n",
    "        saved_forest: List[Dict] = dataclasses.field(default_factory=list, init=False)):\n",
    "\n",
    "        self.ntree = ntree\n",
    "        self.replace = replace\n",
    "        self.sampsize = sampsize\n",
    "        self.sample_fraction = sample_fraction\n",
    "        self.mtry = mtry\n",
    "        self.nodesize_spl = nodesize_spl\n",
    "        self.nodesize_avg = nodesize_avg\n",
    "        self.nodesize_strict_spl = nodesize_strict_spl\n",
    "        self.nodesize_strict_avg = nodesize_strict_avg\n",
    "        self.min_split_gain = min_split_gain\n",
    "        self.max_depth = max_depth\n",
    "        self.interaction_depth = interaction_depth\n",
    "        self.splitratio = splitratio\n",
    "        self.oob_honest = oob_honest\n",
    "        self.double_bootstrap = double_bootstrap\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.nthread = nthread\n",
    "        self.splitrule = splitrule\n",
    "        self.middle_split = middle_split\n",
    "        self.max_obs = max_obs\n",
    "        self.linear = linear\n",
    "        self.min_trees_per_fold = min_trees_per_fold\n",
    "        self.fold_size = fold_size\n",
    "        self.monotone_avg = monotone_avg\n",
    "        self.overfit_penalty = overfit_penalty\n",
    "        self.scale = scale\n",
    "        self.double_tree = double_tree\n",
    "        self.na_direction = na_direction\n",
    "        self.forest = forest\n",
    "        self.dataframe = dataframe\n",
    "        self.processed_dta = processed_dta\n",
    "        self.saved_forest = saved_forest\n",
    "\n",
    "    def fit(self, x: Union[pd.DataFrame, pd.Series, List], y: np.ndarray):\n",
    "        check_X_y(x, y)\n",
    "        self._fit(pd.DataFrame(x), y)\n",
    "        return self\n",
    "\n",
    "    @FitValidator\n",
    "    def _fit(\n",
    "        self,\n",
    "        x: Union[pd.DataFrame, pd.Series, List],\n",
    "        y: np.ndarray,\n",
    "        *,\n",
    "        interaction_variables: Optional[List] = None,  # pylint: disable=unused-argument\n",
    "        feature_weights: Optional[np.ndarray] = None,\n",
    "        deep_feature_weights: Optional[np.ndarray] = None,\n",
    "        observation_weights: Optional[np.ndarray] = None,\n",
    "        lin_feats: Optional[Union[np.ndarray, List]] = None,  # Add a default value.\n",
    "        monotonic_constraints: Optional[np.ndarray] = None,  # Add a default value.\n",
    "        groups: Optional[pd.Series] = None,\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        # Make sure that all the parameters exist when passed to RandomForest\n",
    "\n",
    "        feat_names = get_feat_names(x)\n",
    "\n",
    "        x = (pd.DataFrame(x)).copy()\n",
    "        y = (np.array(y, dtype=np.double)).copy()\n",
    "\n",
    "        nrow, ncol = x.shape\n",
    "\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = round(nrow / 2) + 1\n",
    "\n",
    "        if self.interaction_depth is None:\n",
    "            self.interaction_depth = self.max_depth\n",
    "\n",
    "        if self.max_obs is None:\n",
    "            self.max_obs = y.size\n",
    "\n",
    "        self.sampsize = get_sampsize(self, x)\n",
    "        self.mtry = get_mtry(self, x)\n",
    "\n",
    "        self._set_nodesize_strict()\n",
    "\n",
    "        feature_weights_variables = self._get_weights_variables(feature_weights)\n",
    "        deep_feature_weights_variables = self._get_weights_variables(deep_feature_weights)\n",
    "\n",
    "        feature_weights /= np.sum(feature_weights)\n",
    "        deep_feature_weights /= np.sum(deep_feature_weights)\n",
    "        if self.replace:\n",
    "            observation_weights /= np.sum(observation_weights)\n",
    "\n",
    "        groups_mapping, group_vector = self._get_groups_mapping_and_vector(x, groups)\n",
    "\n",
    "        (\n",
    "            processed_x,\n",
    "            categorical_feature_cols,\n",
    "            categorical_feature_mapping,\n",
    "        ) = preprocess_training(x, y)\n",
    "\n",
    "        if categorical_feature_cols.size != 0:\n",
    "            monotonic_constraints[categorical_feature_cols] = 0\n",
    "\n",
    "        col_means = col_sd = np.repeat(0.0, ncol + 1)\n",
    "        if self.scale:\n",
    "            processed_x, y, col_means, col_sd = scale(x, y, processed_x, categorical_feature_cols)\n",
    "\n",
    "        # cpp linking\n",
    "        processed_x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.dataframe: pd.DataFrame = extension.get_data(\n",
    "            np.ascontiguousarray(pd.concat([processed_x, pd.Series(y)], axis=1).values[:, :], np.double).ravel(),\n",
    "            categorical_feature_cols,\n",
    "            categorical_feature_cols.size,\n",
    "            lin_feats,\n",
    "            lin_feats.size,\n",
    "            feature_weights,\n",
    "            feature_weights_variables,\n",
    "            feature_weights_variables.size,\n",
    "            deep_feature_weights,\n",
    "            deep_feature_weights_variables,\n",
    "            deep_feature_weights_variables.size,\n",
    "            observation_weights,\n",
    "            monotonic_constraints,\n",
    "            group_vector,\n",
    "            self.monotone_avg,\n",
    "            nrow,\n",
    "            ncol + 1,\n",
    "            self._get_seed(seed),\n",
    "        )\n",
    "\n",
    "        self.forest: pd.DataFrame = extension.train_forest(\n",
    "            self.dataframe,\n",
    "            self.ntree,\n",
    "            self.replace,\n",
    "            self.sampsize,\n",
    "            self.splitratio,\n",
    "            self.oob_honest,\n",
    "            self.double_bootstrap,\n",
    "            self.mtry,\n",
    "            self.nodesize_spl,\n",
    "            self.nodesize_avg,\n",
    "            self.nodesize_strict_spl,\n",
    "            self.nodesize_strict_avg,\n",
    "            self.min_split_gain,\n",
    "            self.max_depth,\n",
    "            self.interaction_depth,\n",
    "            self._get_seed(seed),\n",
    "            self.nthread,\n",
    "            self.verbose,\n",
    "            self.middle_split,\n",
    "            self.max_obs,\n",
    "            self.min_trees_per_fold,\n",
    "            self.fold_size,\n",
    "            x.isnull().values.any(),\n",
    "            self.na_direction,\n",
    "            self.linear,\n",
    "            self.overfit_penalty,\n",
    "            self.double_tree,\n",
    "        )\n",
    "\n",
    "        # Update the fields\n",
    "        self.processed_dta = ProcessedDta(\n",
    "            processed_x=processed_x,\n",
    "            y=y,\n",
    "            categorical_feature_cols=categorical_feature_cols,\n",
    "            categorical_feature_mapping=categorical_feature_mapping,\n",
    "            feature_weights=feature_weights,\n",
    "            feature_weights_variables=feature_weights_variables,\n",
    "            deep_feature_weights=deep_feature_weights,\n",
    "            deep_feature_weights_variables=deep_feature_weights_variables,\n",
    "            observation_weights=observation_weights,\n",
    "            monotonic_constraints=monotonic_constraints,\n",
    "            linear_feature_cols=lin_feats,\n",
    "            groups_mapping=groups_mapping,\n",
    "            groups=groups,\n",
    "            col_means=col_means,\n",
    "            col_sd=col_sd,\n",
    "            has_nas=x.isnull().values.any(),\n",
    "            n_observations=nrow,\n",
    "            num_columns=ncol,\n",
    "            feat_names=feat_names,\n",
    "        )\n",
    "\n",
    "    def predict(self, X, *args, **kw):\n",
    "        X = check_array(X)\n",
    "        return self._predict(newdata=X, *args, **kw)\n",
    "\n",
    "    @PredictValidator\n",
    "    def _predict(\n",
    "        self,\n",
    "        newdata: Optional[Union[pd.DataFrame, pd.Series, List]] = None,\n",
    "        *,\n",
    "        aggregation: str = \"average\",\n",
    "        seed: Optional[int] = None,\n",
    "        nthread: Optional[int] = None,\n",
    "        exact: Optional[bool] = None,\n",
    "        trees: Optional[np.ndarray] = None,\n",
    "        training_idx: Optional[np.ndarray] = None,\n",
    "        return_weight_matrix: bool = False,\n",
    "    ) -> Union[np.ndarray, dict]:\n",
    "\n",
    "        if aggregation == \"oob\":\n",
    "            predictions, weight_matrix = self._aggregation_oob(newdata, exact, return_weight_matrix)\n",
    "\n",
    "        elif aggregation == \"doubleOOB\":\n",
    "            predictions, weight_matrix = self._aggregation_double_oob(newdata, exact, return_weight_matrix)\n",
    "\n",
    "        elif aggregation == \"coefs\":\n",
    "            predictions, weight_matrix, coefficients = self._aggregation_coefs(\n",
    "                newdata, exact, self._get_seed(seed), nthread\n",
    "            )\n",
    "            return {\n",
    "                \"predictions\": predictions,\n",
    "                \"coef\": np.lib.stride_tricks.as_strided(\n",
    "                    coefficients,\n",
    "                    shape=(\n",
    "                        self.processed_dta.n_observations,\n",
    "                        self.processed_dta.linear_feature_cols.size + 1,\n",
    "                    ),\n",
    "                    strides=(\n",
    "                        coefficients.itemsize * (self.processed_dta.linear_feature_cols.size + 1),\n",
    "                        coefficients.itemsize,\n",
    "                    ),\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            predictions, weight_matrix, _ = self._aggregation_fallback(\n",
    "                newdata,\n",
    "                exact,\n",
    "                self._get_seed(seed),\n",
    "                nthread,\n",
    "                return_weight_matrix,\n",
    "                trees,\n",
    "            )\n",
    "\n",
    "        if return_weight_matrix:\n",
    "            return {\n",
    "                \"predictions\": predictions,\n",
    "                \"weightMatrix\": np.lib.stride_tricks.as_strided(\n",
    "                    weight_matrix,\n",
    "                    shape=(self._get_n_preds(newdata), self.processed_dta.n_observations),\n",
    "                    strides=(\n",
    "                        weight_matrix.itemsize * self.processed_dta.n_observations,\n",
    "                        weight_matrix.itemsize,\n",
    "                    ),\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X: Union[pd.DataFrame, pd.Series, List], y: np.ndarray, sample_weight: Optional[np.ndarray] = None):\n",
    "        from sklearn.metrics import r2_score\n",
    "        return r2_score(y, self._predict(newdata=X, aggregation=\"average\"), sample_weight=sample_weight)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        return super().set_parameters(**params)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return super().get_parameters()\n",
    "    \n",
    "    def __del__(self):\n",
    "        extension.delete_forestry(self.forest, self.dataframe)\n",
    "        \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        if \"dataframe\" in state:\n",
    "            del state[\"dataframe\"]\n",
    "        if \"forest\" in state:\n",
    "            del state[\"forest\"]\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfbac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:271: SkipTestWarning: Can't test estimator RandomForestEstimator which requires input  of type ['1darray']\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function RandomForestEstimator.__del__ at 0x7fce477c4790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1628/2692302211.py\", line 932, in __del__\n",
      "TypeError: delete_forestry(): incompatible function arguments. The following argument types are supported:\n",
      "    1. (arg0: capsule, arg1: capsule) -> None\n",
      "\n",
      "Invoked with: Field(name=None,type=None,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x7fce7c11d700>,init=False,repr=True,hash=None,compare=True,metadata=mappingproxy({}),_field_type=None), Field(name=None,type=None,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x7fce7c11d700>,init=False,repr=True,hash=None,compare=True,metadata=mappingproxy({}),_field_type=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_estimator(RandomForestEstimator())\n",
    "except TypeError:\n",
    "    print(\"Ain't gonna work as-is. Food for thought to upstream contributors. Let's fix it.\")\n",
    "else:\n",
    "    print(\"Works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98540c32",
   "metadata": {},
   "source": [
    "## Running Modified RForestry Trials w/out Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c113febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials, name=f\"T{n_trials}\")\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = [tune_search_tuning(n_trials) for n_trials in [1, 10, 100]]\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59536eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials, name=f\"T{n_trials}\")\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = [tune_search_tuning(n_trials) for n_trials in [1000]]\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f261338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "@ray.remote(num_cpus=4)\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials, name=f\"T{n_trials}\")\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = ray.get([tune_search_tuning.remote(n_trials) for n_trials in [1, 10, 100]])\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d26312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials)\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = ray.get([tune_search_tuning.remote(n_trial) for n_trial in [1, 10, 100]])\n",
    "\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77249f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials)\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = []\n",
    "\n",
    "#best_params.append(tune_search_tuning(1))\n",
    "#best_params.append(tune_search_tuning(10))\n",
    "#best_params.append(tune_search_tuning(100))\n",
    "\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f33b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials)\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = [tune_search_tuning(1000)]\n",
    "\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662dadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "@ray.remote\n",
    "def tune_search_tuning(n_trials):\n",
    "    data = load_iris()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    X = df.loc[:, df.columns != 'sepal length (cm)']\n",
    "    y = df['sepal length (cm)']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = RandomForestEstimator()\n",
    "    param_distributions = {\n",
    "        \"ntree\": randint(100, 1000)\n",
    "    }\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=n_trials)\n",
    "    tune_search.fit(x_train, y_train)\n",
    "    pred = tune_search.predict(x_test)\n",
    "    return tune_search.score(x_test, y_test)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#best_params = [ray.get(tune_search_tuning.remote(1000))]\n",
    "\n",
    "#print(\"Best Params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388b9cf",
   "metadata": {},
   "source": [
    "## Running SKLearn Random Forest on Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fef418a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "\n",
    "def tune_search_tuning():\n",
    "    digits = datasets.load_digits()\n",
    "    x = digits.data\n",
    "    y = digits.target\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    param_distributions = {\n",
    "        \"n_estimators\": randint(20, 80),\n",
    "        \"max_depth\": randint(2, 10)\n",
    "    }\n",
    "\n",
    "    tune_search = TuneSearchCV(clf, param_distributions, n_trials=1000)\n",
    "\n",
    "    tune_search.fit(x_train, y_train)\n",
    "\n",
    "    pred = tune_search.predict(x_test)\n",
    "    accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
    "    print(accuracy)\n",
    "    return accuracy\n",
    "\n",
    "#start_time = time.time()\n",
    "#remote_clf = tune_search_tuning()\n",
    "#best_params = remote_clf\n",
    "#print(\"Best params\", best_params)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
