[{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sören Künzel. Author. Theo Saarinen. Author, maintainer. Simon Walter. Author. Edward Liu. Author. Allen Tang. Author. Jasjeet Sekhon. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Künzel S, Saarinen T, Walter S, Liu E, Tang , Sekhon J (2023). Rforestry: Random Forests, Linear Trees, Gradient Boosting Inference Interpretability. R package version 0.9.0.157, https://github.com/forestry-labs/Rforestry.","code":"@Manual{,   title = {Rforestry: Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability},   author = {Sören Künzel and Theo Saarinen and Simon Walter and Edward Liu and Allen Tang and Jasjeet Sekhon},   year = {2023},   note = {R package version 0.9.0.157},   url = {https://github.com/forestry-labs/Rforestry}, }"},{"path":"/index.html","id":"rforestry-random-forests-linear-trees-and-gradient-boosting-for-inference-and-interpretability","dir":"","previous_headings":"","what":"Rforestry: Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"Sören Künzel, Theo Saarinen, Simon Walter, Edward Liu, Allen Tang, Jasjeet Sekhon","code":""},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"Rforestry fast implementation Honest Random Forests, Gradient Boosting, Linear Random Forests, emphasis inference interpretability.","code":""},{"path":"/index.html","id":"how-to-install","dir":"","previous_headings":"","what":"How to install","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"GFortran compiler date. GFortran Binaries can found . devtools package installed. can install using, install.packages(\"devtools\"). package contains compiled code, must development environment install development version. can use devtools::has_devel() check whether . development environment exists, Windows users download install Rtools macOS users download install Xcode. latest development version can installed using devtools::install_github(\"forestry-labs/Rforestry\"). Windows users, ’ll need skip 64-bit compilation devtools::install_github(\"forestry-labs/Rforestry\", INSTALL_opts = c('---multiarch')) due outstanding gcc issue.","code":""},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"","code":"library(Rforestry)  set.seed(292315) test_idx <- sample(nrow(iris), 3) x_train <- iris[-test_idx, -1] y_train <- iris[-test_idx, 1] x_test <- iris[test_idx, -1]  rf <- forestry(x = x_train, y = y_train, nthread = 2)  predict(rf, x_test)"},{"path":"/index.html","id":"ridge-random-forest","dir":"","previous_headings":"","what":"Ridge Random Forest","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"fast implementation random forests using ridge penalized splitting ridge regression predictions. order use version random forests, set linear option TRUE.","code":"library(Rforestry)  set.seed(49) n <- c(100) a <- rnorm(n) b <- rnorm(n) c <- rnorm(n) y <- 4*a + 5.5*b - .78*c x <- data.frame(a,b,c) forest <- forestry(x, y, linear = TRUE, nthread = 2) predict(forest, x)"},{"path":"/index.html","id":"monotonic-constraints","dir":"","previous_headings":"","what":"Monotonic Constraints","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"parameter monotonicConstraints strictly enforces monotonicity partition averages evaluating potential splits indicated features. parameter can used specify monotone increasing monotone decreasing constraints.","code":"library(Rforestry)  set.seed(49) x <- rnorm(150)+5 y <- .15*x + .5*sin(3*x) data_train <- data.frame(x1 = x, x2 = rnorm(150)+5, y = y + rnorm(150, sd = .4))  monotone_rf <- forestry(x = data_train[,-3],                         y = data_train$y,                         monotonicConstraints = c(1,1),                         nodesizeStrictSpl = 5,                         nthread = 1,                         ntree = 25)                          predict(monotone_rf, newdata = data_train[,-3])"},{"path":"/index.html","id":"oob-predictions","dir":"","previous_headings":"","what":"OOB Predictions","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"can return predictions training data set using trees observation --bag (OOB). Note trees, high proportion observations sampled, may observations --bag trees. predictions returned NaN. OOB predictions going used, advised one use OOB honesty training (OOBhonest=true). version honesty, OOB observations tree used honest (averaging) set. OOB honesty also changes predictions constructed. predicting observations --sample (using Predict(…, aggregation = “average”)), trees forest used construct predictions. predicting observation -sample (using predict(…, aggregation = “oob”)), trees observation averaging set used construct prediction observation. aggregation=“oob” (--bag) ensures outcome value observation never used construct predictions given observation even sample. property hold standard honesty, relies asymptotic subsampling argument. OOB honesty, used combination aggregation=“oob” prediction stage, overfit IID data, either training prediction stage. outputs models also stable easily interpretable. One can observe one queries model using interpretation tools ALEs, PDPs, LIME, etc.","code":"library(Rforestry)  # Train a forest rf <- forestry(x = iris[,-1],                y = iris[,1],                nthread = 2,                ntree = 500)  # Get the OOB predictions for the training set oob_preds <- predict(rf, aggregation = \"oob\")  # This should be equal to the OOB error mean((oob_preds -  iris[,1])^2) getOOB(rf) library(Rforestry)  # Train a forest rf <- forestry(x = iris[,-1],                y = iris[,1],                nthread = 2,                ntree = 500,                OOBhonest=TRUE)  # Get the OOB predictions for the training set oob_preds <- predict(rf, aggregation = \"oob\")  # This should be equal to the OOB error mean((oob_preds -  iris[,1])^2) getOOB(rf)"},{"path":"/index.html","id":"saving--loading-a-model","dir":"","previous_headings":"","what":"Saving + Loading a model","title":"Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability","text":"order save trained model, include two functions order save load model built. following code shows use saveForestry loadForestry save load forestry model.","code":"library(Rforestry)  # Train a forest forest <- forestry(x = iris[,-1],                    y = iris[,1],                    nthread = 2,                    ntree = 500,                    OOBhonest=TRUE)                 # Get predictions before save the forest y_pred_before <- predict(forest, iris[,-1])  # Save the forest saveForestry(forest, filename = file.path(\"forest.Rda\"))  # Delete the forest rm(forest)  # Load the forest forest_after <- loadForestry(file.path(\"forest.Rda\"))  # Predict after loading the forest y_pred_after <- predict(forest_after, iris[,-1])"},{"path":"/reference/adaptiveForestry-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"forestry with adaptive featureWeights — adaptiveForestry-forestry","title":"forestry with adaptive featureWeights — adaptiveForestry-forestry","text":"experimental function run forestry two   stages, first estimating feature weights calculating relative   splitting proportions feature using small forest,   growing much bigger forest using   first forest splitting proportions featureWeights second forest.","code":""},{"path":"/reference/adaptiveForestry-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"forestry with adaptive featureWeights — adaptiveForestry-forestry","text":"","code":"adaptiveForestry(   x,   y,   ntree = 500,   ntree.first = 25,   ntree.second = 500,   replace = TRUE,   sampsize = if (replace) nrow(x) else ceiling(0.632 * nrow(x)),   sample.fraction = NULL,   mtry = max(floor(ncol(x)/3), 1),   nodesizeSpl = 5,   nodesizeAvg = 5,   nodesizeStrictSpl = 1,   nodesizeStrictAvg = 1,   minSplitGain = 0,   maxDepth = round(nrow(x)/2) + 1,   interactionDepth = maxDepth,   interactionVariables = numeric(0),   featureWeights = NULL,   deepFeatureWeights = NULL,   observationWeights = NULL,   splitratio = 1,   OOBhonest = FALSE,   seed = as.integer(runif(1) * 1000),   verbose = FALSE,   nthread = 0,   splitrule = \"variance\",   middleSplit = FALSE,   maxObs = length(y),   linear = FALSE,   linFeats = 0:(ncol(x) - 1),   monotonicConstraints = rep(0, ncol(x)),   monotoneAvg = FALSE,   overfitPenalty = 1,   scale = FALSE,   doubleTree = FALSE,   reuseforestry = NULL,   savable = TRUE,   saveable = TRUE )"},{"path":"/reference/adaptiveForestry-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"forestry with adaptive featureWeights — adaptiveForestry-forestry","text":"x data frame training predictors. y vector training responses. ntree number trees grow forest. default value 500. ntree.first number trees grow first forest trying determine features important. ntree.second number features use second stage grow second forest using weights first stage. replace indicator whether sampling training data replacement. default value TRUE. sampsize size total samples draw training data. sampling replacement, default value length training data. sampling without replacement, default value two-thirds length training data. sample.fraction given, sampsize ignored set round(length(y) * sample.fraction). must real number 0 1 mtry number variables randomly selected split point. default value set one-third total number features training data. nodesizeSpl Minimum observations contained terminal nodes. default value 5. nodesizeAvg Minimum size terminal nodes averaging dataset. default value 5. nodesizeStrictSpl Minimum observations follow strictly terminal nodes. default value 1. nodesizeStrictAvg minimum size terminal nodes averaging data set follow predicting. splits allowed result nodes observations less parameter. parameter enforces overlap averaging data set splitting set training. using honesty, splits leave less nodesizeStrictAvg averaging observations either child node rejected, ensuring every leaf node also least nodesizeStrictAvg averaging observations. default value 1. minSplitGain Minimum loss reduction split node tree. maxDepth Maximum depth tree. default value 99. interactionDepth splits interaction depth must variables weighting variables (provided interactionVariables argument). interactionVariables Indices weighting variables. featureWeights (optional) vector sampling probabilities/weights feature used subsampling mtry features node interactionDepth. default use uniform probabilities. deepFeatureWeights Used place featureWeights splits interactionDepth. observationWeights Denotes weights training observation determine likely observation selected bootstrap sample. option allowed sampling done without replacement. splitratio Proportion training data used splitting dataset. ratio 0 1. ratio 1 (default), splitting set uses entire data, averaging set---.e., standard Breiman RF setup. ratio 0, splitting data set empty, entire dataset used averaging set (good usage, however, since data available splitting). OOBhonest version honesty, --bag observations tree used honest (averaging) set. setting also changes predictions constructed. predicting observations --sample (using predict(..., aggregation = \"average\")), trees forest used construct predictions. predicting observation -sample (using predict(..., aggregation = \"oob\")), trees observation averaging set used construct prediction observation. aggregation=\"oob\" (--bag) ensures outcome value observation never used construct predictions given observation even sample. property hold standard honesty, relies asymptotic subsampling argument. default, OOBhonest = TRUE, --bag observations tree resamples replacement used honest (averaging) set. results third set observations left splitting averaging set, call double --bag (doubleOOB) observations. order get predictions trees observation fell doubleOOB set, one can run predict(... , aggregation = \"doubleOOB\"). order second bootstrap sample, doubleBootstrap flag can set FALSE. seed random seed verbose Indicator train forest verbose mode nthread Number threads train predict forest. default number 0 represents using cores. splitrule variance implemented point specifies loss function according splits random forest made. middleSplit Indicator whether split value takes average two feature values. FALSE, take point based uniform distribution two feature values. (Default = FALSE) maxObs max number observations split . linear Indicator enables Ridge penalized splits linear aggregation functions leaf nodes. recommended data linear outcomes. implementation details, see: https://arxiv.org/abs/1906.06463. Default FALSE. linFeats vector containing indices features split linearly using linear penalized splits (defaults use numerical features). monotonicConstraints Specifies monotonic relationships continuous features outcome. Supplied vector length p entries 1,0,-1 1 indicating increasing monotonic relationship, -1 indicating decreasing monotonic relationship, 0 indicating constraint. Constraints supplied categorical variable ignored. monotoneAvg boolean flag indicates whether monotonic constraints enforced averaging set addition splitting set. flag meaningless unless honesty monotonic constraints use. default FALSE. overfitPenalty Value determine much penalize magnitude coefficients ridge regression using linear splits. scale parameter indicates whether want scale center covariates outcome regression. can help stability, default TRUE. doubleTree number tree doubled averaging splitting data can exchanged create decorrelated trees. (Default = FALSE) reuseforestry Pass `forestry` object recycle dataframe old object created. save space working data set. savable TRUE, RF created way can saved loaded using save(...) load(...). However, setting TRUE (default) take longer use memory. training many RF, makes sense set FALSE save time memory. saveable deprecated. use.","code":""},{"path":"/reference/adaptiveForestry-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"forestry with adaptive featureWeights — adaptiveForestry-forestry","text":"Two forestry objects, first forest, adaptive forest,   well splitting proportions used grow second forest.","code":""},{"path":"/reference/adaptiveForestry-forestry.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"forestry with adaptive featureWeights — adaptiveForestry-forestry","text":"adaptiveForestry","code":""},{"path":"/reference/adaptiveForestry-forestry.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"forestry with adaptive featureWeights — adaptiveForestry-forestry","text":"","code":"# Set seed for reproductivity set.seed(292313)  # Use Iris Data test_idx <- sample(nrow(iris), 11) x_train <- iris[-test_idx, -1] y_train <- iris[-test_idx, 1] x_test <- iris[test_idx, -1]  rf <- adaptiveForestry(x = x_train,                         y = y_train,                         ntree.first = 25,                         ntree.second = 500,                         nthread = 2) predict(rf@second.forest, x_test) #>  [1] 6.710901 6.569267 4.954476 4.842266 6.171665 5.092438 5.087166 6.068476 #>  [9] 6.470247 5.720181 6.273514"},{"path":"/reference/addTrees-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"addTrees-forestry — addTrees","title":"addTrees-forestry — addTrees","text":"Add trees existing forest.","code":""},{"path":"/reference/addTrees-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addTrees-forestry — addTrees","text":"","code":"addTrees(object, ntree)"},{"path":"/reference/addTrees-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addTrees-forestry — addTrees","text":"object `forestry` object. ntree Number new trees add","code":""},{"path":"/reference/addTrees-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addTrees-forestry — addTrees","text":"`forestry` object","code":""},{"path":"/reference/autoforestry-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"autoforestry-forestry — autoforestry","title":"autoforestry-forestry — autoforestry","text":"autoforestry-forestry","code":""},{"path":"/reference/autoforestry-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"autoforestry-forestry — autoforestry","text":"","code":"autoforestry(   x,   y,   sampsize = as.integer(nrow(x) * 0.75),   num_iter = 1024,   eta = 2,   verbose = FALSE,   seed = 24750371,   nthread = 0 )"},{"path":"/reference/autoforestry-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"autoforestry-forestry — autoforestry","text":"x data frame training predictors. y vector training responses. sampsize size total samples draw training data. num_iter Maximum iterations/epochs per configuration. Default 1024. eta Downsampling rate. Default value 2. verbose tuning process verbose mode seed random seed nthread Number threads train predict forest. default number 0 represents using cores.","code":""},{"path":"/reference/autoforestry-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"autoforestry-forestry — autoforestry","text":"`forestry` object","code":""},{"path":"/reference/autohonestRF.html","id":null,"dir":"Reference","previous_headings":"","what":"Honest Random Forest — autohonestRF","title":"Honest Random Forest — autohonestRF","text":"function deprecated exists backwards   backwards compatibility. function want use `autoforestry`.","code":""},{"path":"/reference/autohonestRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Honest Random Forest — autohonestRF","text":"","code":"autohonestRF(...)"},{"path":"/reference/autohonestRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Honest Random Forest — autohonestRF","text":"... parameters passed directly `autoforestry`","code":""},{"path":"/reference/autohonestRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Honest Random Forest — autohonestRF","text":"`forestry` object","code":""},{"path":"/reference/compute_lp-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"compute lp distances — compute_lp-forestry","title":"compute lp distances — compute_lp-forestry","text":"Return L_p norm distances selected test observations   relative training observations forest trained .","code":""},{"path":"/reference/compute_lp-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"compute lp distances — compute_lp-forestry","text":"","code":"compute_lp(   object,   newdata,   feature,   p,   scale = FALSE,   aggregation = \"average\",   trainingIdx = NULL )"},{"path":"/reference/compute_lp-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"compute lp distances — compute_lp-forestry","text":"object `forestry` object. newdata data frame test predictors. feature string denoting dimension computing lp distances. p positive real number determining norm p-norm used. scale boolean indicating whether want center + scale features (based mean sd training data) calculating L_p norm. useful computing detachment index, can less useful need interpret L_p distances. aggregation aggregation used weightMatrix calculated. can useful calculating lp distances observations training data. must one `average`, `oob`, `doubleOOB`. newdata fewer rows training data, one must also pass vector training indices corresponding indices observations original data set. Default `average`. trainingIdx optional parameter must set aggregation set `oob` `doubleOOB` newdata size training data.","code":""},{"path":"/reference/compute_lp-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"compute lp distances — compute_lp-forestry","text":"vector lp distances.","code":""},{"path":"/reference/compute_lp-forestry.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"compute lp distances — compute_lp-forestry","text":"","code":"# Set seed for reproductivity set.seed(292313)  # Use Iris Data test_idx <- sample(nrow(iris), 11) x_train <- iris[-test_idx, -1] y_train <- iris[-test_idx, 1] x_test <- iris[test_idx, -1]  rf <- forestry(x = x_train, y = y_train,nthread = 2) predict(rf, x_test) #>  [1] 6.656992 6.466264 4.962903 4.863634 6.040959 5.053514 5.095969 6.239164 #>  [9] 6.420248 5.742674 6.238671  # Compute the l2 distances in the \"Petal.Length\" dimension distances_2 <- compute_lp(object = rf,                           newdata = x_test,                           feature = \"Petal.Length\",                           p = 2)"},{"path":"/reference/correctedPredict-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"correctedPredict-forestry — correctedPredict","title":"correctedPredict-forestry — correctedPredict","text":"Perform predictions given forest using bias correction based   bag predictions training set. default use final linear   correction based leave-one-hat matrix `nrounds` nonlinear   corrections.","code":""},{"path":"/reference/correctedPredict-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"correctedPredict-forestry — correctedPredict","text":"","code":"correctedPredict(   object,   newdata,   feats = NULL,   observations = NULL,   nrounds = 0,   linear = TRUE,   binary = FALSE,   aggregation = ifelse(object@OOBhonest, \"doubleOOB\", \"oob\"),   verbose = FALSE,   params.forestry = list(),   keep_fits = FALSE )"},{"path":"/reference/correctedPredict-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"correctedPredict-forestry — correctedPredict","text":"object `forestry` object. newdata Dataframe predict. left NULL, predict sample data. feats vector feature indices included bias correction. default outcome predicted outcomes used. observations vector observation indices original data set used bias correction regression. can used treatment effect estimation carry separate regressions treatment control group. nrounds number nonlinear bias correction steps taken. default zero, just single linear correction used. linear flag indicating whether want final linear bias correction nonlinear corrections. Default TRUE. binary flag indicating whether linear correction use logistic regression binary outcomes. aggregation Gives aggregation use first round predictions. verbose flag displays bias qunatile. params.forestry list parameters pass subsequent forestry calls. Note forests trained features dimension length(feats) + 1 correction forests trained Y ~ cbind(newdata[,feats], Y.hat). monotonic constraints etc given list size length(feats) + 1. Defaults standard forestry parameters parameters included list. keep_fits flag indicates save intermediate forests used bias correction. TRUE, return list forestry objects iteration bias correction.","code":""},{"path":"/reference/correctedPredict-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"correctedPredict-forestry — correctedPredict","text":"vector bias corrected predictions","code":""},{"path":"/reference/correctedPredict-forestry.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"correctedPredict-forestry — correctedPredict","text":"","code":"library(Rforestry)  set.seed(121235312)  n <- 50  p <- 10  x <- matrix(rnorm(n * p), ncol = p)  beta <- runif(p,min = 0, max = 1)  y <- as.matrix(x) %*% beta + rnorm(50)  x <- data.frame(x)   forest <- forestry(x =x,                     y = y[,1],                     OOBhonest = TRUE,                     doubleBootstrap = TRUE)  p <- predict(forest, x)   # Corrected predictions  pred.bc <- correctedPredict(forest,                              newdata = x,                              nrounds = 0)"},{"path":"/reference/CppToR_translator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cpp to R translator — CppToR_translator","title":"Cpp to R translator — CppToR_translator","text":"Add trees existing forest.","code":""},{"path":"/reference/CppToR_translator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cpp to R translator — CppToR_translator","text":"","code":"CppToR_translator(object)"},{"path":"/reference/CppToR_translator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cpp to R translator — CppToR_translator","text":"object external CPP pointer translated Cpp R object","code":""},{"path":"/reference/CppToR_translator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cpp to R translator — CppToR_translator","text":"list lists. sublist contains information span   tree.","code":""},{"path":"/reference/forestry-class.html","id":null,"dir":"Reference","previous_headings":"","what":"forestry class — forestry-class","title":"forestry class — forestry-class","text":"`honestRF` class exists backwards compatibility reasons","code":""},{"path":"/reference/forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"forestry — forestry","title":"forestry — forestry","text":"forestry","code":""},{"path":"/reference/forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"forestry — forestry","text":"","code":"forestry(   x,   y,   ntree = 500,   replace = TRUE,   sampsize = if (replace) nrow(x) else ceiling(0.632 * nrow(x)),   sample.fraction = NULL,   mtry = max(floor(ncol(x)/3), 1),   nodesizeSpl = 5,   nodesizeAvg = 5,   nodesizeStrictSpl = 1,   nodesizeStrictAvg = 1,   minSplitGain = 0,   maxDepth = round(nrow(x)/2) + 1,   interactionDepth = maxDepth,   interactionVariables = numeric(0),   featureWeights = NULL,   deepFeatureWeights = NULL,   observationWeights = NULL,   splitratio = 1,   OOBhonest = FALSE,   doubleBootstrap = if (OOBhonest) TRUE else FALSE,   seed = as.integer(runif(1) * 1000),   verbose = FALSE,   nthread = 0,   splitrule = \"variance\",   middleSplit = FALSE,   maxObs = length(y),   linear = FALSE,   linFeats = 0:(ncol(x) - 1),   monotonicConstraints = rep(0, ncol(x)),   groups = NULL,   minTreesPerFold = 0,   foldSize = 1,   monotoneAvg = FALSE,   overfitPenalty = 1,   scale = TRUE,   doubleTree = FALSE,   naDirection = FALSE,   reuseforestry = NULL,   savable = TRUE,   saveable = TRUE )"},{"path":"/reference/forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"forestry — forestry","text":"x data frame training predictors. y vector training responses. ntree number trees grow forest. default value 500. replace indicator whether sampling training data replacement. default value TRUE. sampsize size total samples draw training data. sampling replacement, default value length training data. sampling without replacement, default value two-thirds length training data. sample.fraction given, sampsize ignored set round(length(y) * sample.fraction). must real number 0 1 mtry number variables randomly selected split point. default value set one-third total number features training data. nodesizeSpl Minimum observations contained terminal nodes. default value 5. nodesizeAvg Minimum size terminal nodes averaging dataset. default value 5. nodesizeStrictSpl Minimum observations follow strictly terminal nodes. default value 1. nodesizeStrictAvg minimum size terminal nodes averaging data set follow predicting. splits allowed result nodes observations less parameter. parameter enforces overlap averaging data set splitting set training. using honesty, splits leave less nodesizeStrictAvg averaging observations either child node rejected, ensuring every leaf node also least nodesizeStrictAvg averaging observations. default value 1. minSplitGain Minimum loss reduction split node tree. maxDepth Maximum depth tree. default value 99. interactionDepth splits interaction depth must variables weighting variables (provided interactionVariables argument). interactionVariables Indices weighting variables. featureWeights (optional) vector sampling probabilities/weights feature used subsampling mtry features node interactionDepth. default use uniform probabilities. deepFeatureWeights Used place featureWeights splits interactionDepth. observationWeights Denotes weights training observation determine likely observation selected bootstrap sample. option allowed sampling done without replacement. splitratio Proportion training data used splitting dataset. ratio 0 1. ratio 1 (default), splitting set uses entire data, averaging set---.e., standard Breiman RF setup. ratio 0, splitting data set empty, entire dataset used averaging set (good usage, however, since data available splitting). OOBhonest version honesty, --bag observations tree used honest (averaging) set. setting also changes predictions constructed. predicting observations --sample (using predict(..., aggregation = \"average\")), trees forest used construct predictions. predicting observation -sample (using predict(..., aggregation = \"oob\")), trees observation averaging set used construct prediction observation. aggregation=\"oob\" (--bag) ensures outcome value observation never used construct predictions given observation even sample. property hold standard honesty, relies asymptotic subsampling argument. default, OOBhonest = TRUE, --bag observations tree resamples replacement used honest (averaging) set. results third set observations left splitting averaging set, call double --bag (doubleOOB) observations. order get predictions trees observation fell doubleOOB set, one can run predict(... , aggregation = \"doubleOOB\"). order second bootstrap sample, doubleBootstrap flag can set FALSE. doubleBootstrap doubleBootstrap flag provides option resample replacement --bag observations set tree construct averaging set using OOBhonest. FALSE, --bag observations used averaging set. default option TRUE running OOBhonest = TRUE. option increases diversity across trees. seed random seed verbose Indicator train forest verbose mode nthread Number threads train predict forest. default number 0 represents using cores. splitrule variance implemented point specifies loss function according splits random forest made. middleSplit Indicator whether split value takes average two feature values. FALSE, take point based uniform distribution two feature values. (Default = FALSE) maxObs max number observations split . linear Indicator enables Ridge penalized splits linear aggregation functions leaf nodes. recommended data linear outcomes. implementation details, see: https://arxiv.org/abs/1906.06463. Default FALSE. linFeats vector containing indices features split linearly using linear penalized splits (defaults use numerical features). monotonicConstraints Specifies monotonic relationships continuous features outcome. Supplied vector length p entries 1,0,-1 1 indicating increasing monotonic relationship, -1 indicating decreasing monotonic relationship, 0 indicating constraint. Constraints supplied categorical variable ignored. groups vector factors specifying group membership training observation. groups used aggregation bag predictions order predict trees entire group used aggregation. allows user specify custom subgroups used create predictions use data common group make predictions observation group. can used create general custom resampling schemes, provide predictions consistent --Group set. minTreesPerFold number trees make sure created leaving fold (fold set randomly selected groups).  0 default, give special treatment groups sampling observations, however set positive integer, modify bootstrap sampling scheme ensure exactly many trees group left . , fold, creating minTreesPerFold trees built observations sampled set training observations group current fold. folds form random partition possible groups, size foldSize. means create least # folds * minTreesPerFold trees forest. ntree > # folds * minTreesPerFold, create max(# folds * minTreesPerFold, ntree) total trees, least minTreesPerFold created leaving fold. foldSize number groups selected randomly fold left using minTreesPerFold. minTreesPerFold set foldSize set, possible groups partitioned folds, containing foldSize unique groups (foldSize evenly divide number groups, single fold smaller, contain remaining groups). minTreesPerFold grown entire fold groups left . monotoneAvg boolean flag indicates whether monotonic constraints enforced averaging set addition splitting set. flag meaningless unless honesty monotonic constraints use. default FALSE. overfitPenalty Value determine much penalize magnitude coefficients ridge regression using linear splits. scale parameter indicates whether want scale center covariates outcome regression. can help stability, default TRUE. doubleTree number tree doubled averaging splitting data can exchanged create decorrelated trees. (Default = FALSE) naDirection Sets default direction missing values split node training. test placing missing values left right, selects direction minimizes loss. missing values exist, default direction randomly selected proportion distribution observations left right. (Default = FALSE) reuseforestry Pass `forestry` object recycle dataframe old object created. save space working data set. savable TRUE, RF created way can saved loaded using save(...) load(...). However, setting TRUE (default) take longer use memory. training many RF, makes sense set FALSE save time memory. saveable deprecated. use.","code":""},{"path":"/reference/forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"forestry — forestry","text":"`forestry` object.","code":""},{"path":"/reference/forestry.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"forestry — forestry","text":"Treatment Missing Data version 0.9.0.34, modified handling missing data. Instead greedy approach used previous iterations, now test potential split putting NA's right, NA's left, taking choice gives best MSE split. version handling potential splits, still respect monotonic constraints. put NA's either side, resulting leaf nodes means violate monotone constraints, split rejected.","code":""},{"path":"/reference/forestry.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"forestry — forestry","text":"","code":"set.seed(292315) library(Rforestry) test_idx <- sample(nrow(iris), 3) x_train <- iris[-test_idx, -1] y_train <- iris[-test_idx, 1] x_test <- iris[test_idx, -1]  rf <- forestry(x = x_train, y = y_train, nthread = 2) predict(rf, x_test) #> [1] 6.521437 5.976053 6.062116  set.seed(49) library(Rforestry)  n <- c(100) a <- rnorm(n) b <- rnorm(n) c <- rnorm(n) y <- 4*a + 5.5*b - .78*c x <- data.frame(a,b,c)  forest <- forestry(           x,           y,           ntree = 10,           replace = TRUE,           nodesizeStrictSpl = 5,           nodesizeStrictAvg = 5,           nthread = 2,           linear = TRUE           )  predict(forest, x) #>   [1]  -4.4602952   1.9185120  -5.3719501   3.1887536   2.5056673  -2.2261169 #>   [7]  -4.4828487   2.5586647   0.1803525  -2.1239150  -0.3284864  -3.1190810 #>  [13] -11.1807256   5.1012522   7.7047888  -2.1203193  -1.9313208  -6.2607888 #>  [19]  10.2222310   5.9168301   4.4130873  -5.2459216   8.3314260   4.4835660 #>  [25]  16.1006414   4.3305739   9.6787632  -2.9951823   2.0429079  -0.3393413 #>  [31]  -7.1960145  -6.4146585   2.1092529  -1.9971309  -3.4679484  -6.8304025 #>  [37]   8.7713721   3.2099896   5.0116151   2.7190026  -8.2033983   0.7842133 #>  [43]  -2.7377068   5.0026544  -0.3831544  -1.1520648  -2.0823994   3.7542273 #>  [49]   5.6839282  -6.4809036   8.2964169  -8.9843952  -6.9026071   5.7371912 #>  [55]   7.2402818  -4.7487601   3.5641563   3.7646351 -16.6895133  -2.4791441 #>  [61]  -1.5121253  -8.2327649   0.9674405  -1.2850284  -9.7787454  -8.8128929 #>  [67]   6.2249640  -6.3294411  -1.1527095   6.7032578   5.1496482  -3.7852007 #>  [73]   1.2553354  -8.8859710  -3.9229119  -7.7582465  -5.5425700  -1.4385328 #>  [79]   7.3466126  -0.2543455   5.4471166  -9.7064159   7.6745907   4.4726340 #>  [85]  -4.1420405   2.4682744   4.6012806   3.2362167   1.6491614  -5.8814158 #>  [91]   4.0261463  12.7383240  -2.4379544  -3.9220895  -5.5355895  -4.5487714 #>  [97]  -8.1041936   2.7816077  -1.4843333  -2.1399396"},{"path":"/reference/forest_checker.html","id":null,"dir":"Reference","previous_headings":"","what":"Checks if forestry object has valid pointer for C++ object. — forest_checker","title":"Checks if forestry object has valid pointer for C++ object. — forest_checker","text":"Checks forestry object valid pointer C++ object.","code":""},{"path":"/reference/forest_checker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Checks if forestry object has valid pointer for C++ object. — forest_checker","text":"","code":"forest_checker(object)"},{"path":"/reference/forest_checker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Checks if forestry object has valid pointer for C++ object. — forest_checker","text":"object forestry object","code":""},{"path":"/reference/forest_checker.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Checks if forestry object has valid pointer for C++ object. — forest_checker","text":"message forest valid C++ pointer.","code":""},{"path":"/reference/getCI-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"getCI-forestry — getCI","title":"getCI-forestry — getCI","text":"new set features, calculate confidence intervals  new observation.","code":""},{"path":"/reference/getCI-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getCI-forestry — getCI","text":"","code":"getCI(   object,   newdata,   level = 0.95,   B = 100,   method = \"OOB-conformal\",   noWarning = FALSE )"},{"path":"/reference/getCI-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getCI-forestry — getCI","text":"object `forestry` object. newdata set new observations want predict outcomes use confidence intervals. level confidence level want make intervals. Default use .95 corresponds 95 percentile confidence intervals. B Number bootstrap draws use using method = \"OOB-bootstrap\" method flag different ways create confidence intervals. Right now two ways . One `OOB-bootstrap` flag uses many bootstrap pulls set OOB trees different pulls, use set trees predict new feature give confidence set many bootstrap draws. method- `OOB-conformal`- creates intervals taking set doubleOOB trees observation, using predictions trees give conformal intervals. observation obs_i, let S_i set trees obs_i neither splitting set averaging set (set trees obs_i \"doubleOOB\"), predict obs_i trees S_i. doubleOOB_tree_preds <- predict(S_i, obs_i): CI(obs_i, alpha = .95) = quantile(doubleOOB_tree_preds - y_i, probs = .95). `local-conformal` option takes residuals training point (using) OOB predictions, uses weights random forest determine quantiles residuals local neighborhood predicted point. Default `OOB-conformal`. noWarning flag display warnings","code":""},{"path":"/reference/getCI-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getCI-forestry — getCI","text":"confidence intervals observation newdata.","code":""},{"path":"/reference/getOOB-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"getOOB-forestry — getOOB-forestry","title":"getOOB-forestry — getOOB-forestry","text":"Calculate --bag error given forest. done using --bag predictions observation, calculating MSE entire forest.","code":""},{"path":"/reference/getOOB-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getOOB-forestry — getOOB-forestry","text":"","code":"getOOB(object, noWarning)"},{"path":"/reference/getOOB-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getOOB-forestry — getOOB-forestry","text":"object `forestry` object. noWarning flag display warnings","code":""},{"path":"/reference/getOOB-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getOOB-forestry — getOOB-forestry","text":"OOB error forest.","code":""},{"path":"/reference/getOOBpreds-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"getOOBpreds-forestry — getOOBpreds-forestry","title":"getOOBpreds-forestry — getOOBpreds-forestry","text":"Calculate --bag predictions given forest.","code":""},{"path":"/reference/getOOBpreds-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getOOBpreds-forestry — getOOBpreds-forestry","text":"","code":"getOOBpreds(object, newdata = NULL, doubleOOB = FALSE, noWarning = FALSE)"},{"path":"/reference/getOOBpreds-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getOOBpreds-forestry — getOOBpreds-forestry","text":"object trained model object class \"forestry\". newdata possible new data frame run bag predictions. NULL, assume indices newdata indices training set, use find trees observation considered /bag . doubleOOB flag specifying whether use double OOB set OOB predictions. set observations tree neither averaging set splitting set. Note forest must trained doubleBootstrap = TRUE used. Default FALSE. noWarning Flag display warnings.","code":""},{"path":"/reference/getOOBpreds-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getOOBpreds-forestry — getOOBpreds-forestry","text":"vector training observations, bag  predictions. Note observation bag different trees,  predictions less stable based observation.  observations may bag trees, predictions  returned NA.","code":""},{"path":[]},{"path":"/reference/getSplitProps-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"getSplitProps-forestry — getSplitProps-forestry","title":"getSplitProps-forestry — getSplitProps-forestry","text":"Retrieves proportion splits feature given  forestry object. proportions calculated number splits  feature entire forest total number splits  forest.","code":""},{"path":"/reference/getSplitProps-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getSplitProps-forestry — getSplitProps-forestry","text":"","code":"getSplitProps(object)"},{"path":"/reference/getSplitProps-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getSplitProps-forestry — getSplitProps-forestry","text":"object trained model object class \"forestry\".","code":""},{"path":"/reference/getSplitProps-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getSplitProps-forestry — getSplitProps-forestry","text":"vector length equal number columns","code":""},{"path":[]},{"path":"/reference/getVI-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"getVI-forestry — getVI","title":"getVI-forestry — getVI","text":"Calculate percentage increase OOB error forest  feature shuffled.","code":""},{"path":"/reference/getVI-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getVI-forestry — getVI","text":"","code":"getVI(object, noWarning, seed = NULL)"},{"path":"/reference/getVI-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getVI-forestry — getVI","text":"object `forestry` object. noWarning flag display warnings seed parameter seed random number generator shuffling features X.","code":""},{"path":"/reference/getVI-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getVI-forestry — getVI","text":"variable importance forest.","code":""},{"path":"/reference/getVI-forestry.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"getVI-forestry — getVI","text":"seed passed function   possible current implementation replicate vector   permutations used measuring feature importance.","code":""},{"path":"/reference/honestRF.html","id":null,"dir":"Reference","previous_headings":"","what":"Honest Random Forest — honestRF","title":"Honest Random Forest — honestRF","text":"function deprecated exists backwards   backwards compatibility. function want use `forestry`.","code":""},{"path":"/reference/honestRF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Honest Random Forest — honestRF","text":"","code":"honestRF(...)"},{"path":"/reference/honestRF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Honest Random Forest — honestRF","text":"... parameters passed directly `forestry`","code":""},{"path":"/reference/honestRF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Honest Random Forest — honestRF","text":"`forestry` object","code":""},{"path":"/reference/impute_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature imputation using random forests neighborhoods — impute_features","title":"Feature imputation using random forests neighborhoods — impute_features","text":"function uses neighborhoods implied random forest   impute missing features. neighbors data point   training points assigned leaf least one tree   forest. weight neighbor fraction trees forest   assigned leaf. impute missing feature   point computing weighted average feature value, using   neighborhood weights, using point's neighbors.","code":""},{"path":"/reference/impute_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature imputation using random forests neighborhoods — impute_features","text":"","code":"impute_features(   object,   newdata,   seed = round(runif(1) * 10000),   use_mean_imputation_fallback = FALSE )"},{"path":"/reference/impute_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature imputation using random forests neighborhoods — impute_features","text":"object object class `forestry` newdata feature data.frame impute missing features . seed random seed passed predict method forestry use_mean_imputation_fallback TRUE, mean imputation (numeric variables) mode imputation (factor variables) used missing features neighbors also corresponding feature missing; FALSE missing features remain NAs data frame returned `impute_features`.","code":""},{"path":"/reference/impute_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature imputation using random forests neighborhoods — impute_features","text":"data.frame newdata imputed missing values.","code":""},{"path":"/reference/impute_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Feature imputation using random forests neighborhoods — impute_features","text":"","code":"iris_with_missing <- iris idx_miss_factor <- sample(nrow(iris), 25, replace = TRUE) iris_with_missing[idx_miss_factor, 5] <- NA idx_miss_numeric <- sample(nrow(iris), 25, replace = TRUE) iris_with_missing[idx_miss_numeric, 3] <- NA  x <- iris_with_missing[,-1] y <- iris_with_missing[, 1]  forest <- forestry(x, y, ntree = 500, seed = 2,nthread = 2) imputed_x <- impute_features(forest, x, seed = 2)"},{"path":"/reference/loadForestry-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"load RF — loadForestry","title":"load RF — loadForestry","text":"wrapper function checks forestry object, makes  saveable needed, saves .","code":""},{"path":"/reference/loadForestry-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"load RF — loadForestry","text":"","code":"loadForestry(filename)"},{"path":"/reference/loadForestry-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"load RF — loadForestry","text":"filename filename store `forestry` object","code":""},{"path":"/reference/loadForestry-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"load RF — loadForestry","text":"loaded forest filename.","code":""},{"path":"/reference/make_savable.html","id":null,"dir":"Reference","previous_headings":"","what":"make_savable — make_savable","title":"make_savable — make_savable","text":"`foresty` object saved reloaded Cpp   pointers data set Cpp forest reconstructed","code":""},{"path":"/reference/make_savable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"make_savable — make_savable","text":"","code":"make_savable(object)"},{"path":"/reference/make_savable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"make_savable — make_savable","text":"object object class `forestry`","code":""},{"path":"/reference/make_savable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"make_savable — make_savable","text":"list lists. sublist contains information span   tree.","code":""},{"path":"/reference/make_savable.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"make_savable — make_savable","text":"`make_savable` translate private member variables   C++ forestry object forest reconstructed   `relinkCPP_prt` attributes lost. example, `nthreads`   reset zero. makes impossible disable threading   predicting forests loaded disk.","code":""},{"path":"/reference/make_savable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"make_savable — make_savable","text":"","code":"set.seed(323652639) x <- iris[, -1] y <- iris[, 1] forest <- forestry(x, y, ntree = 3, nthread = 2) y_pred_before <- predict(forest, x)  forest <- make_savable(forest)  wd <- tempdir() saveForestry(forest, filename = file.path(wd, \"forest.Rda\")) rm(forest)  forest <- loadForestry(file.path(wd, \"forest.Rda\"))  y_pred_after <- predict(forest, x)  file.remove(file.path(wd, \"forest.Rda\")) #> [1] TRUE"},{"path":"/reference/plot-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"visualize a tree — plot-forestry","title":"visualize a tree — plot-forestry","text":"plots tree forest.","code":""},{"path":"/reference/plot-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"visualize a tree — plot-forestry","text":"","code":"# S3 method for forestry plot(x, tree.id = 1, print.meta_dta = FALSE, beta.char.len = 30, ...)"},{"path":"/reference/plot-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"visualize a tree — plot-forestry","text":"x forestry x. tree.id Specifies tree number visualized. print.meta_dta flag indicating whether data plot printed. beta.char.len length beta values leaf node representation. required plotting forestry object linear aggregation functions (linear = TRUE). ... additional arguments used.","code":""},{"path":"/reference/plot-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"visualize a tree — plot-forestry","text":"plot specified tree forest.","code":""},{"path":"/reference/plot-forestry.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"visualize a tree — plot-forestry","text":"","code":"set.seed(292315) rf <- forestry(x = iris[,-1],                y = iris[, 1],                nthread = 2)  plot(x = rf)  {\"x\":{\"nodes\":{\"id\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"shape\":[\"circle\",\"circle\",\"circle\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"box\",\"circle\",\"box\",\"circle\",\"box\",\"box\",\"box\",\"circle\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"box\"],\"label\":[\"Sepal.Width\",\"Petal.Width\",\"Species\",\"5 Obs\\n=======\\nm = 5.32\",\"Petal.Width\",\"28 Obs\\n=======\\nm = 4.71429\",\"1 Obs\\n=======\\nm = 5.1\",\"Petal.Length\",\"Sepal.Width\",\"Petal.Width\",\"Petal.Width\",\"Species\",\"2 Obs\\n=======\\nm = 6.2\",\"30 Obs\\n=======\\nm = 5.95667\",\"2 Obs\\n=======\\nm = 4.9\",\"Sepal.Width\",\"Sepal.Width\",\"5 Obs\\n=======\\nm = 6.04\",\"4 Obs\\n=======\\nm = 6.15\",\"Sepal.Width\",\"3 Obs\\n=======\\nm = 6.3\",\"Petal.Width\",\"6 Obs\\n=======\\nm = 6.15\",\"2 Obs\\n=======\\nm = 6.5\",\"Species\",\"Petal.Width\",\"5 Obs\\n=======\\nm = 6.76\",\"Sepal.Width\",\"1 Obs\\n=======\\nm = 6.7\",\"Petal.Length\",\"3 Obs\\n=======\\nm = 6.4\",\"2 Obs\\n=======\\nm = 6.3\",\"7 Obs\\n=======\\nm = 6.58571\",\"Petal.Length\",\"9 Obs\\n=======\\nm = 6.94444\",\"Petal.Length\",\"3 Obs\\n=======\\nm = 7.7\",\"2 Obs\\n=======\\nm = 7.45\",\"Petal.Width\",\"Sepal.Width\",\"Sepal.Width\",\"16 Obs\\n=======\\nm = 5.15625\",\"Petal.Width\",\"2 Obs\\n=======\\nm = 4.9\",\"3 Obs\\n=======\\nm = 5.1\",\"Petal.Width\",\"1 Obs\\n=======\\nm = 5.5\",\"4 Obs\\n=======\\nm = 5.55\",\"4 Obs\\n=======\\nm = 7.025\"],\"level\":[1,2,3,4,4,5,5,3,4,5,6,7,8,8,7,6,7,8,8,7,8,8,9,9,5,6,7,7,8,8,9,9,6,4,5,5,6,6,2,3,4,5,5,6,6,4,5,5,3],\"title\":[\"Sepal.Width\",\"Petal.Width\",\"Species\",\"5 Obs\",\"Petal.Width\",\"28 Obs\",\"1 Obs\",\"Petal.Length\",\"Sepal.Width\",\"Petal.Width\",\"Petal.Width\",\"Species\",\"2 Obs\",\"30 Obs\",\"2 Obs\",\"Sepal.Width\",\"Sepal.Width\",\"5 Obs\",\"4 Obs\",\"Sepal.Width\",\"3 Obs\",\"Petal.Width\",\"6 Obs\",\"2 Obs\",\"Species\",\"Petal.Width\",\"5 Obs\",\"Sepal.Width\",\"1 Obs\",\"Petal.Length\",\"3 Obs\",\"2 Obs\",\"7 Obs\",\"Petal.Length\",\"9 Obs\",\"Petal.Length\",\"3 Obs\",\"2 Obs\",\"Petal.Width\",\"Sepal.Width\",\"Sepal.Width\",\"16 Obs\",\"Petal.Width\",\"2 Obs\",\"3 Obs\",\"Petal.Width\",\"1 Obs\",\"4 Obs\",\"4 Obs\"],\"color\":[\"#E6E600B3\",\"#EEB99FB3\",\"#F2F2F2B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#E6E600B3\",\"#EEB99FB3\",\"#EEB99FB3\",\"#F2F2F2B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#E6E600B3\",\"#E6E600B3\",\"#00A600B3\",\"#00A600B3\",\"#E6E600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#F2F2F2B3\",\"#EEB99FB3\",\"#00A600B3\",\"#E6E600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#E6E600B3\",\"#E6E600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\"]},\"edges\":{\"from\":[1,2,3,3,5,5,2,8,9,10,11,12,12,11,10,16,17,17,16,20,20,22,22,9,25,26,26,28,28,30,30,25,8,34,34,36,36,1,39,40,41,41,43,43,40,46,46,39],\"to\":[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"smooth.enabled\":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],\"smooth.type\":[\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\"],\"smooth.roundness\":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],\"label\":[\" < 0.61\",\" < -0.11\",\" = versicolor\",\" != versicolor\",\" < -0.94\",\" >= -0.94\",\" >= -0.11\",\" < 1.07\",\" < -0.05\",\" < 0.73\",\" < 0.52\",\" = virginica\",\" != virginica\",\" >= 0.52\",\" >= 0.73\",\" < -0.45\",\" < -0.7\",\" >= -0.7\",\" >= -0.45\",\" < -0.16\",\" >= -0.16\",\" < 1.03\",\" >= 1.03\",\" >= -0.05\",\" = versicolor\",\" < 0.35\",\" >= 0.35\",\" < 0.18\",\" >= 0.18\",\" < 0.44\",\" >= 0.44\",\" != versicolor\",\" >= 1.07\",\" < 1.3\",\" >= 1.3\",\" < 1.42\",\" >= 1.42\",\" >= 0.61\",\" < 0.69\",\" < 1.9\",\" < 1.1\",\" >= 1.1\",\" < -1.18\",\" >= -1.18\",\" >= 1.9\",\" < -1.09\",\" >= -1.09\",\" >= 0.69\"],\"width\":[3.2,0.906666666666667,0.133333333333333,0.773333333333333,0.746666666666667,0.0266666666666667,2.29333333333333,1.92,1.44,0.906666666666667,0.853333333333333,0.0533333333333333,0.8,0.0533333333333333,0.533333333333333,0.24,0.133333333333333,0.106666666666667,0.293333333333333,0.08,0.213333333333333,0.16,0.0533333333333333,0.48,0.293333333333333,0.133333333333333,0.16,0.0266666666666667,0.133333333333333,0.08,0.0533333333333333,0.186666666666667,0.373333333333333,0.24,0.133333333333333,0.08,0.0533333333333333,0.8,0.693333333333333,0.56,0.426666666666667,0.133333333333333,0.0533333333333333,0.08,0.133333333333333,0.0266666666666667,0.106666666666667,0.106666666666667]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\"},\"manipulation\":{\"enabled\":false},\"edges\":{\"arrows\":\"to\"},\"layout\":{\"hierarchical\":{\"enabled\":true}}},\"groups\":null,\"width\":\"100%\",\"height\":\"800px\",\"idselection\":{\"enabled\":false},\"byselection\":{\"enabled\":false},\"main\":{\"text\":\"Tree 1\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"export\":{\"type\":\"pdf\",\"css\":\"float:right;-webkit-border-radius: 10;\\n                  -moz-border-radius: 10;\\n                  border-radius: 10px;\\n                  font-family: Arial;\\n                  color: #ffffff;\\n                  font-size: 12px;\\n                  background: #090a0a;\\n                  padding: 4px 8px 4px 4px;\\n                  text-decoration: none;\",\"background\":\"#fff\",\"name\":\"ridge_tree.pdf\",\"label\":\"Export as pdf\"}},\"evals\":[],\"jsHooks\":[]}plot(x = rf, tree.id = 2)  {\"x\":{\"nodes\":{\"id\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51],\"shape\":[\"circle\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"circle\",\"box\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"box\"],\"label\":[\"Petal.Width\",\"Sepal.Width\",\"Petal.Width\",\"Petal.Length\",\"3 Obs\\n=======\\nm = 5.06667\",\"4 Obs\\n=======\\nm = 5.725\",\"2 Obs\\n=======\\nm = 5.65\",\"Petal.Length\",\"27 Obs\\n=======\\nm = 4.77778\",\"Petal.Length\",\"Petal.Width\",\"Petal.Length\",\"Sepal.Width\",\"Petal.Width\",\"1 Obs\\n=======\\nm = 4.9\",\"5 Obs\\n=======\\nm = 4.88\",\"Petal.Width\",\"1 Obs\\n=======\\nm = 5.2\",\"4 Obs\\n=======\\nm = 5.35\",\"4 Obs\\n=======\\nm = 4.825\",\"4 Obs\\n=======\\nm = 5.15\",\"Petal.Width\",\"4 Obs\\n=======\\nm = 5.325\",\"2 Obs\\n=======\\nm = 5.1\",\"Petal.Length\",\"Petal.Width\",\"Species\",\"Sepal.Width\",\"Sepal.Width\",\"Sepal.Width\",\"1 Obs\\n=======\\nm = 6.3\",\"4 Obs\\n=======\\nm = 5.675\",\"Sepal.Width\",\"8 Obs\\n=======\\nm = 6.125\",\"12 Obs\\n=======\\nm = 6.06667\",\"Petal.Length\",\"Petal.Length\",\"2 Obs\\n=======\\nm = 6.3\",\"3 Obs\\n=======\\nm = 6.03333\",\"8 Obs\\n=======\\nm = 6.675\",\"25 Obs\\n=======\\nm = 6.084\",\"Petal.Length\",\"3 Obs\\n=======\\nm = 6.3\",\"10 Obs\\n=======\\nm = 6.7\",\"Petal.Width\",\"Sepal.Width\",\"Petal.Width\",\"5 Obs\\n=======\\nm = 7.34\",\"3 Obs\\n=======\\nm = 7.7\",\"4 Obs\\n=======\\nm = 7.75\",\"1 Obs\\n=======\\nm = 7.2\"],\"level\":[1,2,3,4,5,5,4,3,4,4,5,6,7,8,9,9,8,9,9,7,6,5,6,6,2,3,4,5,6,7,8,8,7,8,8,6,7,8,8,7,5,4,5,5,3,4,5,6,6,5,4],\"title\":[\"Petal.Width\",\"Sepal.Width\",\"Petal.Width\",\"Petal.Length\",\"3 Obs\",\"4 Obs\",\"2 Obs\",\"Petal.Length\",\"27 Obs\",\"Petal.Length\",\"Petal.Width\",\"Petal.Length\",\"Sepal.Width\",\"Petal.Width\",\"1 Obs\",\"5 Obs\",\"Petal.Width\",\"1 Obs\",\"4 Obs\",\"4 Obs\",\"4 Obs\",\"Petal.Width\",\"4 Obs\",\"2 Obs\",\"Petal.Length\",\"Petal.Width\",\"Species\",\"Sepal.Width\",\"Sepal.Width\",\"Sepal.Width\",\"1 Obs\",\"4 Obs\",\"Sepal.Width\",\"8 Obs\",\"12 Obs\",\"Petal.Length\",\"Petal.Length\",\"2 Obs\",\"3 Obs\",\"8 Obs\",\"25 Obs\",\"Petal.Length\",\"3 Obs\",\"10 Obs\",\"Petal.Width\",\"Sepal.Width\",\"Petal.Width\",\"5 Obs\",\"3 Obs\",\"4 Obs\",\"1 Obs\"],\"color\":[\"#EEB99FB3\",\"#E6E600B3\",\"#EEB99FB3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#EAB64EB3\",\"#EEB99FB3\",\"#EAB64EB3\",\"#E6E600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#EEB99FB3\",\"#F2F2F2B3\",\"#E6E600B3\",\"#E6E600B3\",\"#E6E600B3\",\"#00A600B3\",\"#00A600B3\",\"#E6E600B3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#E6E600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\"]},\"edges\":{\"from\":[1,2,3,4,4,3,2,8,8,10,11,12,13,14,14,13,17,17,12,11,10,22,22,1,25,26,27,28,29,30,30,29,33,33,28,36,37,37,36,27,26,42,42,25,45,46,47,47,46,45],\"to\":[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51],\"smooth.enabled\":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],\"smooth.type\":[\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\"],\"smooth.roundness\":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],\"label\":[\" < 0.06\",\" < -0.77\",\" < -0.05\",\" < -0.03\",\" >= -0.03\",\" >= -0.05\",\" >= -0.77\",\" < -1.32\",\" >= -1.32\",\" < -1.19\",\" < -1.23\",\" < -1.22\",\" < 1.13\",\" < -1.33\",\" >= -1.33\",\" >= 1.13\",\" < -1.38\",\" >= -1.38\",\" >= -1.22\",\" >= -1.23\",\" >= -1.19\",\" < -1.15\",\" >= -1.15\",\" >= 0.06\",\" < 1.14\",\" < 1.11\",\" = versicolor\",\" < -0.34\",\" < -0.65\",\" < -1.57\",\" >= -1.57\",\" >= -0.65\",\" < -0.4\",\" >= -0.4\",\" >= -0.34\",\" < 0.49\",\" < 0.41\",\" >= 0.41\",\" >= 0.49\",\" != versicolor\",\" >= 1.11\",\" < 0.93\",\" >= 0.93\",\" >= 1.14\",\" < 1.65\",\" < 1.04\",\" < 1.39\",\" >= 1.39\",\" >= 1.04\",\" >= 1.65\"],\"width\":[1.62666666666667,0.24,0.186666666666667,0.08,0.106666666666667,0.0533333333333333,1.38666666666667,0.72,0.666666666666667,0.506666666666667,0.4,0.293333333333333,0.16,0.0266666666666667,0.133333333333333,0.133333333333333,0.0266666666666667,0.106666666666667,0.106666666666667,0.106666666666667,0.16,0.106666666666667,0.0533333333333333,2.37333333333333,2.02666666666667,1.68,1.01333333333333,0.666666666666667,0.133333333333333,0.0266666666666667,0.106666666666667,0.533333333333333,0.213333333333333,0.32,0.346666666666667,0.133333333333333,0.0533333333333333,0.08,0.213333333333333,0.666666666666667,0.346666666666667,0.08,0.266666666666667,0.346666666666667,0.32,0.213333333333333,0.133333333333333,0.08,0.106666666666667,0.0266666666666667]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\"},\"manipulation\":{\"enabled\":false},\"edges\":{\"arrows\":\"to\"},\"layout\":{\"hierarchical\":{\"enabled\":true}}},\"groups\":null,\"width\":\"100%\",\"height\":\"800px\",\"idselection\":{\"enabled\":false},\"byselection\":{\"enabled\":false},\"main\":{\"text\":\"Tree 2\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"export\":{\"type\":\"pdf\",\"css\":\"float:right;-webkit-border-radius: 10;\\n                  -moz-border-radius: 10;\\n                  border-radius: 10px;\\n                  font-family: Arial;\\n                  color: #ffffff;\\n                  font-size: 12px;\\n                  background: #090a0a;\\n                  padding: 4px 8px 4px 4px;\\n                  text-decoration: none;\",\"background\":\"#fff\",\"name\":\"ridge_tree.pdf\",\"label\":\"Export as pdf\"}},\"evals\":[],\"jsHooks\":[]}plot(x = rf, tree.id = 500)  {\"x\":{\"nodes\":{\"id\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65],\"shape\":[\"circle\",\"circle\",\"circle\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"circle\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"circle\",\"box\",\"box\",\"box\",\"box\",\"box\",\"circle\",\"box\",\"box\"],\"label\":[\"Species\",\"Petal.Width\",\"Petal.Length\",\"9 Obs\\n=======\\nm = 4.53333\",\"Petal.Width\",\"4 Obs\\n=======\\nm = 4.85\",\"34 Obs\\n=======\\nm = 4.93824\",\"Petal.Length\",\"5 Obs\\n=======\\nm = 5.34\",\"Sepal.Width\",\"Sepal.Width\",\"1 Obs\\n=======\\nm = 5.1\",\"4 Obs\\n=======\\nm = 5\",\"1 Obs\\n=======\\nm = 5.4\",\"Sepal.Width\",\"Sepal.Width\",\"Petal.Width\",\"Sepal.Width\",\"Petal.Width\",\"3 Obs\\n=======\\nm = 4.96667\",\"3 Obs\\n=======\\nm = 5.66667\",\"Sepal.Width\",\"Petal.Width\",\"3 Obs\\n=======\\nm = 6.43333\",\"4 Obs\\n=======\\nm = 5.7\",\"Species\",\"2 Obs\\n=======\\nm = 6.1\",\"Petal.Length\",\"1 Obs\\n=======\\nm = 5.2\",\"Petal.Length\",\"Sepal.Width\",\"3 Obs\\n=======\\nm = 5.6\",\"4 Obs\\n=======\\nm = 5.7\",\"1 Obs\\n=======\\nm = 6\",\"1 Obs\\n=======\\nm = 7.7\",\"Species\",\"Petal.Length\",\"Petal.Width\",\"12 Obs\\n=======\\nm = 6.225\",\"Sepal.Width\",\"Petal.Width\",\"6 Obs\\n=======\\nm = 6.4\",\"1 Obs\\n=======\\nm = 5.8\",\"Sepal.Width\",\"Sepal.Width\",\"3 Obs\\n=======\\nm = 6.96667\",\"4 Obs\\n=======\\nm = 6.825\",\"Petal.Length\",\"2 Obs\\n=======\\nm = 6.25\",\"3 Obs\\n=======\\nm = 6.43333\",\"Petal.Length\",\"2 Obs\\n=======\\nm = 7.7\",\"3 Obs\\n=======\\nm = 7.56667\",\"Petal.Length\",\"Petal.Width\",\"Petal.Length\",\"Petal.Width\",\"4 Obs\\n=======\\nm = 5.7\",\"4 Obs\\n=======\\nm = 5.775\",\"2 Obs\\n=======\\nm = 6.2\",\"6 Obs\\n=======\\nm = 5.65\",\"9 Obs\\n=======\\nm = 6.7\",\"Petal.Width\",\"4 Obs\\n=======\\nm = 7.85\",\"2 Obs\\n=======\\nm = 7.2\"],\"level\":[1,2,3,4,4,5,5,3,4,4,5,6,6,5,2,3,4,5,6,7,7,6,7,8,8,7,8,8,9,9,10,11,11,10,5,4,5,6,7,7,8,9,9,8,9,10,10,9,10,10,6,7,7,5,6,7,8,9,9,8,7,6,3,4,4],\"title\":[\"Species\",\"Petal.Width\",\"Petal.Length\",\"9 Obs\",\"Petal.Width\",\"4 Obs\",\"34 Obs\",\"Petal.Length\",\"5 Obs\",\"Sepal.Width\",\"Sepal.Width\",\"1 Obs\",\"4 Obs\",\"1 Obs\",\"Sepal.Width\",\"Sepal.Width\",\"Petal.Width\",\"Sepal.Width\",\"Petal.Width\",\"3 Obs\",\"3 Obs\",\"Sepal.Width\",\"Petal.Width\",\"3 Obs\",\"4 Obs\",\"Species\",\"2 Obs\",\"Petal.Length\",\"1 Obs\",\"Petal.Length\",\"Sepal.Width\",\"3 Obs\",\"4 Obs\",\"1 Obs\",\"1 Obs\",\"Species\",\"Petal.Length\",\"Petal.Width\",\"12 Obs\",\"Sepal.Width\",\"Petal.Width\",\"6 Obs\",\"1 Obs\",\"Sepal.Width\",\"Sepal.Width\",\"3 Obs\",\"4 Obs\",\"Petal.Length\",\"2 Obs\",\"3 Obs\",\"Petal.Length\",\"2 Obs\",\"3 Obs\",\"Petal.Length\",\"Petal.Width\",\"Petal.Length\",\"Petal.Width\",\"4 Obs\",\"4 Obs\",\"2 Obs\",\"6 Obs\",\"9 Obs\",\"Petal.Width\",\"4 Obs\",\"2 Obs\"],\"color\":[\"#F2F2F2B3\",\"#EEB99FB3\",\"#EAB64EB3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#E6E600B3\",\"#E6E600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#E6E600B3\",\"#E6E600B3\",\"#EEB99FB3\",\"#E6E600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#E6E600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#F2F2F2B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#EAB64EB3\",\"#E6E600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#F2F2F2B3\",\"#EAB64EB3\",\"#EEB99FB3\",\"#00A600B3\",\"#E6E600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#E6E600B3\",\"#E6E600B3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#00A600B3\",\"#00A600B3\",\"#EAB64EB3\",\"#EEB99FB3\",\"#EAB64EB3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#00A600B3\",\"#EEB99FB3\",\"#00A600B3\",\"#00A600B3\"]},\"edges\":{\"from\":[1,2,3,3,5,5,2,8,8,10,11,11,10,1,15,16,17,18,19,19,18,22,23,23,22,26,26,28,28,30,31,31,30,17,16,36,37,38,38,40,41,41,40,44,45,45,44,48,48,37,51,51,36,54,55,56,57,57,56,55,54,15,63,63],\"to\":[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65],\"smooth.enabled\":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],\"smooth.type\":[\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\"],\"smooth.roundness\":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],\"label\":[\" = setosa\",\" < -1.14\",\" < -1.36\",\" >= -1.36\",\" < -1.38\",\" >= -1.38\",\" >= -1.14\",\" < -1.25\",\" >= -1.25\",\" < 1.3\",\" < 0.74\",\" >= 0.74\",\" >= 1.3\",\" != setosa\",\" < 0.99\",\" < -0.69\",\" < 1.38\",\" < -1.41\",\" < -0.14\",\" >= -0.14\",\" >= -1.41\",\" < -1.23\",\" < 0.99\",\" >= 0.99\",\" >= -1.23\",\" = virginica\",\" != virginica\",\" < 0.1\",\" >= 0.1\",\" < 0.66\",\" < -0.9\",\" >= -0.9\",\" >= 0.66\",\" >= 1.38\",\" >= -0.69\",\" = virginica\",\" < 1.32\",\" < 1.07\",\" >= 1.07\",\" < -0.19\",\" < 1.51\",\" >= 1.51\",\" >= -0.19\",\" < 0.47\",\" < -0.03\",\" >= -0.03\",\" >= 0.47\",\" < 1.07\",\" >= 1.07\",\" >= 1.32\",\" < 1.4\",\" >= 1.4\",\" != virginica\",\" < 0.43\",\" < 0.37\",\" < 0.36\",\" < 0.12\",\" >= 0.12\",\" >= 0.36\",\" >= 0.37\",\" >= 0.43\",\" >= 0.99\",\" < 1.61\",\" >= 1.61\"],\"width\":[1.54666666666667,1.25333333333333,0.24,1.01333333333333,0.106666666666667,0.906666666666667,0.293333333333333,0.133333333333333,0.16,0.133333333333333,0.0266666666666667,0.106666666666667,0.0266666666666667,2.45333333333333,2.29333333333333,0.666666666666667,0.64,0.16,0.08,0.08,0.48,0.186666666666667,0.08,0.106666666666667,0.293333333333333,0.0533333333333333,0.24,0.0266666666666667,0.213333333333333,0.186666666666667,0.08,0.106666666666667,0.0266666666666667,0.0266666666666667,1.62666666666667,0.96,0.826666666666667,0.32,0.506666666666667,0.186666666666667,0.16,0.0266666666666667,0.32,0.186666666666667,0.08,0.106666666666667,0.133333333333333,0.0533333333333333,0.08,0.133333333333333,0.0533333333333333,0.08,0.666666666666667,0.426666666666667,0.266666666666667,0.213333333333333,0.106666666666667,0.106666666666667,0.0533333333333333,0.16,0.24,0.16,0.106666666666667,0.0533333333333333]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\"},\"manipulation\":{\"enabled\":false},\"edges\":{\"arrows\":\"to\"},\"layout\":{\"hierarchical\":{\"enabled\":true}}},\"groups\":null,\"width\":\"100%\",\"height\":\"800px\",\"idselection\":{\"enabled\":false},\"byselection\":{\"enabled\":false},\"main\":{\"text\":\"Tree 500\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"export\":{\"type\":\"pdf\",\"css\":\"float:right;-webkit-border-radius: 10;\\n                  -moz-border-radius: 10;\\n                  border-radius: 10px;\\n                  font-family: Arial;\\n                  color: #ffffff;\\n                  font-size: 12px;\\n                  background: #090a0a;\\n                  padding: 4px 8px 4px 4px;\\n                  text-decoration: none;\",\"background\":\"#fff\",\"name\":\"ridge_tree.pdf\",\"label\":\"Export as pdf\"}},\"evals\":[],\"jsHooks\":[]}"},{"path":"/reference/predict-adaptiveForestry.html","id":null,"dir":"Reference","previous_headings":"","what":"predict-adaptiveForestry — predict-adaptiveForestry","title":"predict-adaptiveForestry — predict-adaptiveForestry","text":"Return prediction forest.","code":""},{"path":"/reference/predict-adaptiveForestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"predict-adaptiveForestry — predict-adaptiveForestry","text":"","code":"# S3 method for adaptiveForestry predict(   object,   newdata,   aggregation = \"average\",   seed = as.integer(runif(1) * 10000),   nthread = 0,   exact = NULL,   weighting = NULL,   ... )"},{"path":"/reference/predict-adaptiveForestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"predict-adaptiveForestry — predict-adaptiveForestry","text":"object `adaptiveForestry` object. newdata data frame testing predictors. aggregation individual tree predictions aggregated: `average` returns mean trees forest; `weightMatrix` returns list consisting \"weightMatrix\", adaptive nearest neighbor weights used construct predictions; \"terminalNodes\", matrix ith entry jth column index leaf node ith observation assigned jth tree; \"sparse\", matrix ith entry jth column 1 ith observation feature.new assigned jth leaf 0 otherwise. tree leaves indexed using depth first ordering, , \"sparse\" representation, first leaf second tree column index one number leaves first tree . , example, first tree 5 leaves, sixth column \"sparse\" matrix corresponds first leaf second tree. seed random seed nthread number threads run predictions . default number threads forest trained . exact specifies whether forest predictions aggregated reproducible ordering. Due non-associativity floating point addition, predict parallel, predictions aggregated varied orders different threads finish different times. default, exact TRUE unless N > 100,000 custom aggregation function used. weighting number 0 1 indicating weight use predictions two forests. specifically specifies weight given second.forest object. predictions given weighting * predict(object@second.forest) + (1-weighting) * predict(object@first.forest). Defaults NULL, case, weighting = ntree.second / (ntree.first + ntree.second). ... additional arguments.","code":""},{"path":"/reference/predict-adaptiveForestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"predict-adaptiveForestry — predict-adaptiveForestry","text":"vector predicted responses.","code":""},{"path":"/reference/predict-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"predict-forestry — predict-forestry","title":"predict-forestry — predict-forestry","text":"Return prediction forest.","code":""},{"path":"/reference/predict-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"predict-forestry — predict-forestry","text":"","code":"# S3 method for forestry predict(   object,   newdata = NULL,   aggregation = \"average\",   holdOutIdx = NULL,   trainingIdx = NULL,   seed = as.integer(runif(1) * 10000),   nthread = 0,   exact = NULL,   trees = NULL,   weightMatrix = FALSE,   ... )"},{"path":"/reference/predict-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"predict-forestry — predict-forestry","text":"object `forestry` object. newdata data frame testing predictors. aggregation individual tree predictions aggregated: `average` returns mean trees forest; `terminalNodes` also returns weightMatrix, well \"terminalNodes\", matrix ith entry jth column index leaf node ith observation assigned jth tree; \"sparse\", matrix ith entry jth column 1 ith observation newdata assigned jth leaf 0 otherwise. tree leaves indexed using depth first ordering, , \"sparse\" representation, first leaf second tree column index one number leaves first tree . , example, first tree 5 leaves, sixth column \"sparse\" matrix corresponds first leaf second tree. `oob` returns --bag predictions forest. assume ordering observations newdata changed training. ordering changed, get wrong OOB indices. `doubleOOB` experimental flag, can used OOBhonest = TRUE doubleBootstrap = TRUE. settings , splitting set selected bootstrap sample observations averaging set selected bootstrap sample observations left bag splitting set selection. leaves third set observations selected either bootstrap sample. predict flag gives predictions using- observation- trees observation fell third set (neither splitting averaging example). `coefs` aggregation option works linear aggregation functions used. returns linear coefficients linear feature used leaf node regression predicted point. holdOutIdx optional argument, containing vector indices training data set allowed influence predictions forest. vector indices training observations given, predictions made trees forest contain indices either splitting averaging sets. used time aggregation options. `weightMatrix = TRUE`, return weightMatrix corresponding predictions made trees respecting holdOutIdx. trees held indices holdOutIdx, predictions return NaN. trainingIdx optional parameter give indices observations `newdata` training data set. used want run predict subset observations training data set use `aggregation = \"oob\"` `aggregation = \"doubleOOB\"`. example, tree level, tree make bag (`aggregation = \"oob\"`) predictions indices set setdiff(trainingIdx,tree$averagingIndices) make double --bag predictions indices set setdiff(trainingIdx,union(tree$averagingIndices,tree$splittingIndices). Note, parameter must set predict called --bag aggregation option data set matching original training data size. order indices `trainingIdx` also needs match order observations newdata. arbitrary index set `trainingIdx` dataframe `newdata`,  size training set, predictions `predict(rf, newdata[trainingIdx,],` `aggregation = \"oob\", trainingIdx = trainingIdx)` match predictions `predict(rf, newdata, aggregation = \"oob\")[trainingIdx]`. option also works `weightMatrix` option return (smaller) weightMatrix observations passed data frame. seed random seed nthread number threads run predictions . default number threads forest trained . exact specifies whether forest predictions aggregated reproducible ordering. Due non-associativity floating point addition, predict parallel, predictions aggregated varied orders different threads finish different times. default, exact TRUE unless N > 100,000 custom aggregation function used. trees vector (1-indexed) indices range 1:ntree tells predict trees forest use prediction. Predict default take average trees forest, although flag can used get single tree predictions, averages diffferent trees different weightings. Duplicate entries allowed, trees = c(1,2,2) predict weighted average prediction trees 1 2 weighted : predict(..., trees = c(1,2,2)) = (predict(..., trees = c(1)) +                                    2*predict(..., trees = c(2))) / 3. note must exact = TRUE, aggregation = \"average\" use tree indices. weightMatrix indicator whether also return matrix weights given training observation making prediction. getting weight matrix, aggregation must one `average`, `oob`, `doubleOOB`. ... additional arguments.","code":""},{"path":"/reference/predict-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"predict-forestry — predict-forestry","text":"vector predicted responses.","code":""},{"path":"/reference/predictInfo-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"predictInfo-forestry — predictInfo","title":"predictInfo-forestry — predictInfo","text":"Get observations used predict set new  observations using either trees (sample observations),  tree observation averaging set sample entirely.","code":""},{"path":"/reference/predictInfo-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"predictInfo-forestry — predictInfo","text":"","code":"predictInfo(object, newdata, aggregation = \"average\")"},{"path":"/reference/predictInfo-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"predictInfo-forestry — predictInfo","text":"object `forestry` object. newdata Data want predictions. Must length training set `oob` `doubleOOB` aggregation. aggregation Specifies aggregation version used predict observation, must one `average`,`oob`, `doubleOOB`.","code":""},{"path":"/reference/predictInfo-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"predictInfo-forestry — predictInfo","text":"list four entries. `weightMatrix` matrix specifying  weight given training observatio prediction observation j.  `avgIndices` gives indices averaging set new  observation. `avgWeights` gives weights corresponding averaging  observation returned `avgIndices`. `obsInfo` gives full observation vectors  used predict observation, well weight given  observation.","code":""},{"path":"/reference/preprocess_testing.html","id":null,"dir":"Reference","previous_headings":"","what":"preprocess_testing — preprocess_testing","title":"preprocess_testing — preprocess_testing","text":"Perform preprocessing testing data, including converting   data dataframe, testing columns consistent   training data encoding categorical data numerical representation   way training data.","code":""},{"path":"/reference/preprocess_testing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"preprocess_testing — preprocess_testing","text":"","code":"preprocess_testing(x, categoricalFeatureCols, categoricalFeatureMapping)"},{"path":"/reference/preprocess_testing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"preprocess_testing — preprocess_testing","text":"x data frame training predictors. categoricalFeatureCols list index categorical data. Used trees detect categorical columns. categoricalFeatureMapping list encoding details categorical column, including unique factor values corresponding numeric representation.","code":""},{"path":"/reference/preprocess_testing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"preprocess_testing — preprocess_testing","text":"preprocessed training dataaset x","code":""},{"path":"/reference/preprocess_training.html","id":null,"dir":"Reference","previous_headings":"","what":"preprocess_training — preprocess_training","title":"preprocess_training — preprocess_training","text":"Perform preprocessing training data, including   converting data dataframe, encoding categorical data numerical   representation.","code":""},{"path":"/reference/preprocess_training.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"preprocess_training — preprocess_training","text":"","code":"preprocess_training(x, y)"},{"path":"/reference/preprocess_training.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"preprocess_training — preprocess_training","text":"x data frame training predictors. y vector training responses.","code":""},{"path":"/reference/preprocess_training.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"preprocess_training — preprocess_training","text":"list two datasets along necessary information encodes   preprocessing.","code":""},{"path":"/reference/relinkCPP.html","id":null,"dir":"Reference","previous_headings":"","what":"relink CPP ptr — relinkCPP_prt","title":"relink CPP ptr — relinkCPP_prt","text":"`foresty` object saved reloaded Cpp   pointers data set Cpp forest reconstructed","code":""},{"path":"/reference/relinkCPP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"relink CPP ptr — relinkCPP_prt","text":"","code":"relinkCPP_prt(object)"},{"path":"/reference/relinkCPP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"relink CPP ptr — relinkCPP_prt","text":"object object class `forestry`","code":""},{"path":"/reference/relinkCPP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"relink CPP ptr — relinkCPP_prt","text":"Relinks pointer correct C++ object.","code":""},{"path":"/reference/saveForestry-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"save RF — saveForestry","title":"save RF — saveForestry","text":"wrapper function checks forestry object, makes  saveable needed, saves .","code":""},{"path":"/reference/saveForestry-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"save RF — saveForestry","text":"","code":"saveForestry(object, filename, ...)"},{"path":"/reference/saveForestry-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"save RF — saveForestry","text":"object object class `forestry` filename filename store `forestry` object ... additional arguments useful specifying compression type level","code":""},{"path":"/reference/saveForestry-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"save RF — saveForestry","text":"Saves forest filename.","code":""},{"path":"/reference/scale_center.html","id":null,"dir":"Reference","previous_headings":"","what":"scale_center — scale_center","title":"scale_center — scale_center","text":"Given dataframe, scale center continous features","code":""},{"path":"/reference/scale_center.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"scale_center — scale_center","text":"","code":"scale_center(x, categoricalFeatureCols, colMeans, colSd)"},{"path":"/reference/scale_center.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"scale_center — scale_center","text":"x dataframe order processed. categoricalFeatureCols vector categorical features, want scale/center . 1-indexed. colMeans vector means center column. colSd vector standard deviations scale column .","code":""},{"path":"/reference/scale_center.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scale_center — scale_center","text":"scaled centered  dataset x","code":""},{"path":"/reference/testing_data_checker-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"Test data check — testing_data_checker-forestry","title":"Test data check — testing_data_checker-forestry","text":"Check testing data prediction","code":""},{"path":"/reference/testing_data_checker-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test data check — testing_data_checker-forestry","text":"","code":"testing_data_checker(object, newdata, hasNas)"},{"path":"/reference/testing_data_checker-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test data check — testing_data_checker-forestry","text":"object forestry object. newdata data frame testing predictors. hasNas TRUE NAs training data FALSE otherwise.","code":""},{"path":"/reference/testing_data_checker-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test data check — testing_data_checker-forestry","text":"feature dataframe can used new predictions.","code":""},{"path":"/reference/training_data_checker-forestry.html","id":null,"dir":"Reference","previous_headings":"","what":"Training data check — training_data_checker","title":"Training data check — training_data_checker","text":"Check input forestry constructor","code":""},{"path":"/reference/training_data_checker-forestry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Training data check — training_data_checker","text":"","code":"training_data_checker(   x,   y,   ntree,   replace,   sampsize,   mtry,   nodesizeSpl,   nodesizeAvg,   nodesizeStrictSpl,   nodesizeStrictAvg,   minSplitGain,   maxDepth,   interactionDepth,   splitratio,   OOBhonest,   nthread,   middleSplit,   doubleTree,   linFeats,   monotonicConstraints,   groups,   featureWeights,   deepFeatureWeights,   observationWeights,   linear,   scale,   hasNas,   naDirection )"},{"path":"/reference/training_data_checker-forestry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Training data check — training_data_checker","text":"x data frame training predictors. y vector training responses. ntree number trees grow forest. default value 500. replace indicator whether sampling training data replacement. default value TRUE. sampsize size total samples draw training data. sampling replacement, default value length training data. sampling without replacement, default value two-thirds length training data. mtry number variables randomly selected split point. default value set one-third total number features training data. nodesizeSpl Minimum observations contained terminal nodes. default value 5. nodesizeAvg Minimum size terminal nodes averaging dataset. default value 5. nodesizeStrictSpl Minimum observations follow strictly terminal nodes. default value 1. nodesizeStrictAvg minimum size terminal nodes averaging data set follow predicting. splits allowed result nodes observations less parameter. parameter enforces overlap averaging data set splitting set training. using honesty, splits leave less nodesizeStrictAvg averaging observations either child node rejected, ensuring every leaf node also least nodesizeStrictAvg averaging observations. default value 1. minSplitGain Minimum loss reduction split node tree. maxDepth Maximum depth tree. default value 99. interactionDepth splits interaction depth must variables weighting variables (provided interactionVariables argument). splitratio Proportion training data used splitting dataset. ratio 0 1. ratio 1 (default), splitting set uses entire data, averaging set---.e., standard Breiman RF setup. ratio 0, splitting data set empty, entire dataset used averaging set (good usage, however, since data available splitting). OOBhonest version honesty, --bag observations tree used honest (averaging) set. setting also changes predictions constructed. predicting observations --sample (using predict(..., aggregation = \"average\")), trees forest used construct predictions. predicting observation -sample (using predict(..., aggregation = \"oob\")), trees observation averaging set used construct prediction observation. aggregation=\"oob\" (--bag) ensures outcome value observation never used construct predictions given observation even sample. property hold standard honesty, relies asymptotic subsampling argument. default, OOBhonest = TRUE, --bag observations tree resamples replacement used honest (averaging) set. results third set observations left splitting averaging set, call double --bag (doubleOOB) observations. order get predictions trees observation fell doubleOOB set, one can run predict(... , aggregation = \"doubleOOB\"). order second bootstrap sample, doubleBootstrap flag can set FALSE. nthread Number threads train predict forest. default number 0 represents using cores. middleSplit Indicator whether split value takes average two feature values. FALSE, take point based uniform distribution two feature values. (Default = FALSE) doubleTree number tree doubled averaging splitting data can exchanged create decorrelated trees. (Default = FALSE) linFeats vector containing indices features split linearly using linear penalized splits (defaults use numerical features). monotonicConstraints Specifies monotonic relationships continuous features outcome. Supplied vector length p entries 1,0,-1 1 indicating increasing monotonic relationship, -1 indicating decreasing monotonic relationship, 0 indicating constraint. Constraints supplied categorical variable ignored. groups vector factors specifying group membership training observation. groups used aggregation bag predictions order predict trees entire group used aggregation. allows user specify custom subgroups used create predictions use data common group make predictions observation group. can used create general custom resampling schemes, provide predictions consistent --Group set. featureWeights weights used subsampling features nodes interactionDepth. deepFeatureWeights weights used subsampling features nodes interactionDepth. observationWeights Denotes weights training observation determine likely observation selected bootstrap sample. option allowed sampling done without replacement. linear Indicator enables Ridge penalized splits linear aggregation functions leaf nodes. recommended data linear outcomes. implementation details, see: https://arxiv.org/abs/1906.06463. Default FALSE. scale parameter indicates whether want scale center covariates outcome regression. can help stability, default TRUE. hasNas indicates missingness x. naDirection Sets default direction missing values split node training. test placing missing values left right, selects direction minimizes loss. missing values exist, default direction randomly selected proportion distribution observations left right. (Default = FALSE)","code":""},{"path":"/reference/training_data_checker-forestry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Training data check — training_data_checker","text":"list parameters checking selected parameters valid.","code":""},{"path":"/reference/unscale_uncenter.html","id":null,"dir":"Reference","previous_headings":"","what":"unscale_uncenter — unscale_uncenter","title":"unscale_uncenter — unscale_uncenter","text":"Given dataframe, un scale un center continous features","code":""},{"path":"/reference/unscale_uncenter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"unscale_uncenter — unscale_uncenter","text":"","code":"unscale_uncenter(x, categoricalFeatureCols, colMeans, colSd)"},{"path":"/reference/unscale_uncenter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"unscale_uncenter — unscale_uncenter","text":"x dataframe order processed. categoricalFeatureCols vector categorical features, want scale/center . 1-indexed. colMeans vector means add column. colSd vector standard deviations rescale column .","code":""},{"path":"/reference/unscale_uncenter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"unscale_uncenter — unscale_uncenter","text":"dataset x original scaling","code":""}]
